<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>样例组</title>
      <link href="/2024/01/01/GroupTest/"/>
      <url>/2024/01/01/GroupTest/</url>
      
        <content type="html"><![CDATA[<p><font color="red" size="3">注：</font><br><font color="red" size="4"> 除以下内容外，还需一张封面图片及摘要（至多六十三字或136个字符）</font><br><font color="red" size="4"> 摘要可自定义的内容，若无，则程序会自动截取文章的部分内容作为摘要</font><br><font color="red" size="4"> 文章发布时间仅作为排序使用，与实际发布时间无关 </font>  </p><h2 id="项目内容"><a href="#项目内容" class="headerlink" title="项目内容"></a>项目内容</h2><h3 id="项目名称-（必填）"><a href="#项目名称-（必填）" class="headerlink" title="项目名称 （必填）"></a>项目名称 <font color="red" size="3">（必填）</font></h3><p>样例</p><h3 id="项目简介-（必填）"><a href="#项目简介-（必填）" class="headerlink" title="项目简介 （必填）"></a>项目简介 <font color="red" size="3">（必填）</font></h3><p>本组为样例组，主要为展示可提交的素材类型及需求<br>需上传项目进度或有疑问可联系我：<br>微信：18020020921<br>QQ：1678400948  </p><hr><h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="小组成员-（必填）"><a href="#小组成员-（必填）" class="headerlink" title="小组成员 （必填）"></a>小组成员 <font color="red" size="3">（必填）</font></h3><p>张三</p><h3 id="组员照片-（选填）"><a href="#组员照片-（选填）" class="headerlink" title="组员照片 （选填）"></a>组员照片 <font color="blue" size="3">（选填）</font></h3><p><img src="/./images/test.jpeg" alt="测试图片"></p><h3 id="组员分工-（必填）"><a href="#组员分工-（必填）" class="headerlink" title="组员分工 （必填）"></a>组员分工 <font color="red" size="3">（必填）</font></h3><hr><h2 id="项目进度-（必填，大概一周一次）"><a href="#项目进度-（必填，大概一周一次）" class="headerlink" title="项目进度 （必填，大概一周一次）"></a>项目进度 <font color="red" size="3">（必填，大概一周一次）</font></h2><h3 id="第一周-（图片选填）"><a href="#第一周-（图片选填）" class="headerlink" title="第一周 （图片选填）"></a>第一周 <font color="blue" size="3">（图片选填）</font></h3><p><img src="/./images/test2.png" alt="测试图片"></p><h3 id="第二周-（代码选填）"><a href="#第二周-（代码选填）" class="headerlink" title="第二周 （代码选填）"></a>第二周 <font color="blue" size="3">（代码选填）</font></h3><pre class="line-numbers language-某语言" data-language="某语言"><code class="language-某语言">我是代码<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 样例 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Implementing Cat and Dog Classification Based on Pytorch</title>
      <link href="/2023/10/31/GroupOne/"/>
      <url>/2023/10/31/GroupOne/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Implementing Cat and Dog Classification Based on Pytorch<br>基于Pytorch实现猫狗分类</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>猫狗分类项目是一个经典的图像分类任务，它旨在通过训练一个深度学习模型来区分图像中的内容是猫还是狗。在这个项目中，我们将使用PyTorch这个流行的深度学习框架来实现。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>李好 谭静雅</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="项目讲解"><a href="#项目讲解" class="headerlink" title="项目讲解"></a>项目讲解</h4><ol><li>数据准备<br>首先，我们需要准备一个包含猫和狗图片的数据集。这个数据集通常被划分为训练集、验证集和测试集。训练集用于训练模型，验证集用于调整模型参数和超参数，测试集用于评估模型的性能。</li><li>数据预处理<br>在将数据输入模型之前，我们需要对数据进行一些预处理操作。这通常包括：</li></ol><p>图像大小调整：将图像调整到模型输入所需的尺寸。<br>归一化：将图像的像素值缩放到一个特定的范围（通常是0到1之间），以加速模型的训练。<br>数据增强：通过随机旋转、裁剪、翻转等操作增加训练数据的多样性，提高模型的泛化能力。</p><ol start="3"><li><p>模型构建<br>在PyTorch中，我们可以使用nn.Module来构建自己的神经网络模型。对于猫狗分类任务，一个常见的选择是使用卷积神经网络（CNN）。CNN能够有效地从图像中提取特征，并通过全连接层进行分类。</p></li><li><p>定义损失函数和优化器<br>损失函数：用于衡量模型预测结果与真实标签之间的差异。对于分类任务，常用的损失函数包括交叉熵损失（CrossEntropyLoss）。<br>优化器：用于更新模型的权重以最小化损失函数。常用的优化器包括随机梯度下降（SGD）、Adam等。</p></li><li><p>训练模型<br>在训练阶段，我们将多次遍历训练集，每次从训练集中取出一批数据，计算损失，并通过优化器更新模型的权重。为了监控模型的训练过程，我们还可以在验证集上计算模型的性能指标（如准确率）。</p></li><li><p>评估模型<br>在模型训练完成后，我们使用测试集来评估模型的性能。通常，我们会计算模型在测试集上的准确率、精确率、召回率等指标。</p></li><li><p>模型部署与应用<br>最后，我们可以将训练好的模型部署到实际应用中，用于对新的图像进行猫狗分类。这通常涉及到将模型导出为可执行的格式，并在实际应用中加载和使用模型进行预测。</p></li></ol><p>注意事项<br>过拟合与欠拟合：在训练过程中，要注意防止模型出现过拟合或欠拟合的情况。可以通过调整模型复杂度、增加正则化项、使用早停法等方法来缓解这些问题。<br>超参数调整：模型的性能往往受到超参数（如学习率、批次大小、训练轮数等）的影响。可以通过网格搜索、随机搜索或贝叶斯优化等方法来找到最佳的超参数组合。<br>硬件资源：深度学习模型的训练通常需要大量的计算资源，包括GPU和内存。因此，在进行猫狗分类项目时，要确保有足够的硬件资源来支持模型的训练和部署。</p><h4 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h4><p>一、环境配置<br>1、安装Anaconda<br>Anaconda是一个用于科学计算的python发行版，它提供了包管理和环境管理的功能。在官网根据python版本进行选择。<br>2、配置Pytorch<br>创建名为pytorch的虚拟环境<br>(base) C:\Users\XYtentrary&gt;conda create -n pytorch python=3.9<br>在此环境下安装<br>(pytorch) C:\Users\XYtentrary&gt;conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=11.1 -c pytorch -c conda-forge<br>二、数据集的准备<br>1、数据集的下载<br>通过kaggle网站进行对所需数据集的下载<br><a href="https://www.kaggle.com/lizhensheng/-2000">https://www.kaggle.com/lizhensheng/-2000</a><br>2.数据集的分类<br>将下载的数据集放入名为data的文件夹下并进行解压操作<br>得到的数据集分为test和train两个文件夹。（文件夹下包含cats和dogs图片）<br><img src="/images/1/1-1.png"><br>3.导入相应的库<br>以下是用pytorch进行分类时需导入的库的大致类别：<br>import torch<br>import torchvision<br>import torch.nn as nn<br>import torch.optim as optim<br>import torch.utils.data as data</p><h4 id="项目计划"><a href="#项目计划" class="headerlink" title="项目计划"></a>项目计划</h4><p>一、导入库<br>二、加载数据<br>三、构建模型<br>四、训练模型<br>五、实现分类预测测试<br>六、预测结果展示与分析</p><h4 id="项目进展汇报"><a href="#项目进展汇报" class="headerlink" title="项目进展汇报"></a>项目进展汇报</h4><p>一、导入相应的库<br>import matplotlib.pyplot as plt<br>import torch<br>import torch.nn as nn<br>from torch.optim import Adam<br>from torchvision import transforms<br>from torchvision import models<br>from torchvision.io import read_image<br>from torch.nn import functional as F<br>from torch.utils.data import Dataset, DataLoader<br>import numpy as np<br>from sklearn.model_selection import StratifiedShuffleSplit<br>import os<br>os.environ[‘TORCH_HOME’]=’./download_model’  # 修改Pytorch模型默认下载位置</p><p>二、加载数据<br>数据预处理<br>在基于Pytorch实现猫狗分类任务时，对数据进行适当的预处理是至关重要的。以下是对数据进行预处理的详细步骤：即可以完成对猫狗分类数据集的基本预处理。<br>·获取train目录下的文件路径名<br><img src="/images/1/1-2.png"><br>·使用 StratifiedShuffleSplit 进行随机分层抽样<br>将25000张图片按 8:1:1 的比例分为训练集、验证集和测试集，且每个数据集中猫和狗的占比均相同<br><img src="/images/1/1-3.png"><br><img src="/images/1/1-4.png"><br>·定义Pytorch数据加载类（定义Dataset）<br><img src="/images/1/1-5.png"><br>·定义一些指标<br> -image_size = [224, 224]  # 图片大小<br> batch_size = 20# 批大小<br>·定义图像增强器<br><img src="/images/1/1-6.png"><br><img src="/images/1/1-7.png"><br><img src="/images/1/1-8.png"><br><img src="/images/1/1-9.png"><br><img src="/images/1/1-10.png"><br>三、构建模型<br>模型选择<br>使用残差神经网络(ResNet)效果最佳，其极大的消除了深度过大的神经网络训练困难问题，可以让更深的网络也可以得到更好的训练效果。<br>模型搭建<br>·对预训练的ResNet50模型进行部分修改，将顶层的1000类输出替换为2类，并冻结参数。(在当前文件夹下新建download_model文件夹，因为导入库时我们使用os将Pytorch的目录重定向到download_model，ResNet50将会下载到这个文件夹内)<br><img src="/images/1/1-11.png"><br>·打印出模型<br>·将模型迁移到GPU上提高训练速度<br><img src="/images/1/1-12.png"><br>·定义优化器和损失函数<br><img src="/images/1/1-13.png"><br><img src="/images/1/1-14.png"></p><h4 id="下一步行动"><a href="#下一步行动" class="headerlink" title="下一步行动"></a>下一步行动</h4><p>四、训练模型<br>五、实现分类预测测试<br>六、预测结果展示与分析</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周-使用迁移学习识别多种鸟类"><a href="#第十二周-使用迁移学习识别多种鸟类" class="headerlink" title="第十二周  使用迁移学习识别多种鸟类"></a>第十二周  使用迁移学习识别多种鸟类</h3><h4 id="01课题背景和意义"><a href="#01课题背景和意义" class="headerlink" title="01课题背景和意义"></a>01课题背景和意义</h4><p>1.鸟类识别的重要性：<br>随着环境保护意识的增强和生态研究的深入，鸟类的识别和监测成为了生态保护和研究中的重要环节。<br>2.传统识别方法的局限：<br>传统的鸟类识别方法通常依赖于专家的经验和人工识别，效率低且易出错。<br>3.迁移学习在鸟类识别中的应用：<br>现代科技发展为鸟类识别带来了新的解决方案，特别是迁移学习在图像识别中的应用。<br>4.高效鸟类识别算法系统的价值：<br>开发准确、高效的鸟类识别算法系统，助力生态学家、环保人士和鸟类爱好者识别和监测鸟类种群，促进生物多样性和生态环境研究。</p><h4 id="02实验目的"><a href="#02实验目的" class="headerlink" title="02实验目的"></a>02实验目的</h4><p>1.实验目的：<br>利用迁移学习技术，采用预训练的深度学习模型（ResNet101）来识别和分类不同种类的鸟类。<br>2.模型微调：<br>通过微调预训练模型的部分层，探索迁移学习在生物多样性监测领域的应用效果。</p><h4 id="03实验环境"><a href="#03实验环境" class="headerlink" title="03实验环境"></a>03实验环境</h4><p>1.编程语言：Python 3<br>2.深度学习框架：PyTorch, torchvision<br>3.硬件：使用CUDA-enabled GPU进行加速计算，无GPU时使用CPU</p><h4 id="04实验理论基础"><a href="#04实验理论基础" class="headerlink" title="04实验理论基础"></a>04实验理论基础</h4><p>迁移学习<br>1.迁移学习简介：<br>迁移学习涉及把在旧任务上训练好的模型进行修改，再用新任务的数据进行训练，使之能完成新任务。<br>2.迁移学习优点：<br>迁移学习利用预训练模型，提高学习效率和性能，解决数据不足问题，加速开发过程，并增强模型的泛化能力。<br>3.迁移学习应用：<br>迁移学习支持多任务学习场景，同一个模型可以适应多个相关任务，提高效率，并可能提升每个任务的性能。</p><p>残差网络<br>1.残差网络简介：<br>残差网络（ResNet）通过增加直连边的方式来提高信息传播效率，类似于高速公路，可以使得信息更快地传递。<br>2.目标函数拆分：<br>将目标函数拆分成恒等函数和残差函数两部分，根据通用近似定理，神经网络有足够能力来近似逼近原始目标函数或残差函数。<br>3.优化问题转换：<br>由于残差函数更容易学习，因此将原来的优化问题转换为让非线性单元去近似残差函数，用去逼近恒等函数。<br>4.残差单元结构：<br>典型的残差单元由多个级联的（等宽）卷积层和一个跨层的直连边组成，再经过ReLU激活后得到输出。<br>5.残差网络结构：<br>残差网络是由多个残差单元串联起来构成的非常深的网络，类似于Highway Network。</p><h4 id="05数据集描述"><a href="#05数据集描述" class="headerlink" title="05数据集描述"></a>05数据集描述</h4><p>1.数据集来源：<br>实验使用的数据集CUB-200来自加州理工学院，涵盖200种鸟类，共计6033张图片。<br>2.数据集下载：<br>CUB-200数据集可从官网下载，下载并解压缩后，将image文件夹复制到代码运行目录的data文件夹内。</p><h4 id="06数据预处理"><a href="#06数据预处理" class="headerlink" title="06数据预处理"></a>06数据预处理</h4><p>1.数据预处理：<br>包括随机裁剪、随机水平翻转和标准化处理，有助于增强模型对位置和尺寸变化的鲁棒性，并扩展数据的多样性。<br>2.验证数据预处理：<br>包括缩放到256<em>256像素，中心裁剪到224</em>224像素，及标准化处理，确保数据符合模型输入要求。</p><h4 id="07模型构建"><a href="#07模型构建" class="headerlink" title="07模型构建"></a>07模型构建</h4><p>1.选择ResNet101模型：<br>因为其深度和广泛的应用背景，且模型预训练于ImageNet数据集，具备强大的特征提取能力。<br>2.替换全连接层：<br>原始的全连接层针对1000个类别的ImageNet任务设计，为适应新的分类任务，需要替换为新的全连接层，其输出节点数为目标鸟类种类的数量。<br>3.参数调整：<br>在训练过程中，初期可以冻结除全连接层以外的所有层的权重，专注于训练新的全连接层，以适应新的任务，同时保持已学习的特征不变。</p><h4 id="08训练策略"><a href="#08训练策略" class="headerlink" title="08训练策略"></a>08训练策略</h4><p>第一阶段（微调全连接层）<br>1.适应新分类任务：<br>要快速适应新分类任务，可以通过只训练全连接层来实现，而固定其他参数。<br>2.优化器选择：<br>选择使用Adam优化器，其自适应学习率的特性适合快速微调，从而提高模型性能。<br>3.学习率设置：<br>将学习率设置为0.001，旨在在不破坏预训练特征的情况下更新权重，确保模型稳定训练。</p><p>第二阶段（全面训练）<br>1.优化模型：<br>要优化模型，在全局范围内提高鸟类识别任务的性能，切换到SGD优化器进行深度和广泛的微调。<br>2.学习率调度：<br>通过使用学习率调度器，初始学习率设置为0.001，每2个epoch衰减10%，以避免过拟合。<br>3.解冻层设置：<br>所有层的权重解冻，允许模型在整体上进行调整，以更好地适应数据特征。</p><h4 id="09实验步骤"><a href="#09实验步骤" class="headerlink" title="09实验步骤"></a>09实验步骤</h4><p>1.数据加载：<br>使用自定义数据加载器读取训练和验证数据，为模型训练和评估做好准备。<br>2.模型训练：<br>在两个阶段分别进行模型训练，记录每个epoch的损失和精度，确保模型性能。<br>3.模型评估：<br>在验证集上评估模型，计算和记录总体准确率和类别准确率，以评估模型性能。</p><h4 id="10实验结果（代码运行结果）"><a href="#10实验结果（代码运行结果）" class="headerlink" title="10实验结果（代码运行结果）"></a>10实验结果（代码运行结果）</h4><p>实现<br>1.加载图片文件名称和标签<br>load_data函数用于加载图片文件名称和标签，方便后续的数据处理和模型训练。<br>2.引入基础库<br>加载图片文件名称和标签函数中引入了glob、os、numpy、PIL、matplotlib.pyplot等基础库。<br>3.定义load_dir函数<br>load_dir函数用于获取指定目录中的所有图片和标签，方便数据的组织和处理。<br>4.对标签进行排序<br>为了确保训练和验证的顺序一致性，需要对标签进行排序，以便按照相同的顺序进行处理。<br>5.创建文件标签列表<br>通过遍历标签列表，并使用glob模块匹配对应的jpg文件名，构建文件标签列表。</p><p>实现自定义数据集类<br>1.自定义数据集类<br>OwnDataset类用于加载和处理自定义数据集，支持缓存和索引列表。<br>2.初始化函数<br>init函数用于初始化数据集，设置标签、图片目录、转换函数和加载函数。<br>3.getitem函数<br>getitem函数用于根据索引获取数据，支持缓存和转换。<br>4.缓存机制<br>数据首先被加载到内存中，以便快速访问，提高效率。<br>5.索引列表<br>通过索引列表，用户可以自定义数据的加载顺序，实现数据的定制化加载。</p><p>测试数据集<br>1.定义数据的预处理方法<br>‘train’和’val’分别对应训练集和验证集的数据预处理方法，包括随机裁剪、随机水平翻转、转换为tensor、标准化等操作。<br>2.加载数据的方法<br>load_data函数用于加载数据集，返回文件名、标签和类别名。其中，filename是文件列表，label是对应的标签列表，classes是类别名列表。<br>3.还原图片的方法<br>Reduction_img函数用于将归一化后的图片张量还原为原始图片，通过扩展维度、计算均值和标准差，并使用torch.as_tensor将结果转换为tensor。<br>4.打乱数组顺序的方法<br>使用np.random.seed(0)来设置随机种子，以确保每次打乱数组顺序时得到相同的结果。label_shuffle_index是打乱后的标签索引列表。</p><p>获取并改造ResNET模型<br>1.指定设备<br>通过torch.device函数指定设备为GPU或CPU，根据是否有可用的CUDA来决定。<br>2.获取ResNet模型<br>使用torchvision.models.resnet101函数获取预训练的ResNet模型，或通过loadfile参数加载本地的模型。<br>3.冻结参数层<br>通过循环遍历ResNet的所有参数层，并将requires_grad属性设置为False，以冻结这些参数层。<br>4.修改全连接层<br>打印出ResNet的全连接层信息，获取fc层的输入特征数，并定义一个新的全连接层。<br>5.返回改造后的模型<br>返回改造后的ResNet模型，该模型已适应新的分类任务。</p><p>微调模型最后一层<br>1.指定fc层学习率<br>通过<code>nn.CrossEntropyLoss()</code>指定新加的fc层的学习率，使用<code>torch.optim.Adam</code>优化器进行训练。<br>2.训练模型<br>定义<code>train</code>函阿数，用于训练模型，包括设置模型为训练模式、清除梯度、前向传播、计算损失、反向传播和更新权重等步骤。<br>3.测试模型<br>定义<code>test</code>函数，用于测试模型，包括设置模型为评估模式、初始化测试损失和正确率、迭代验证集并计算损失和预测值等步骤。</p><p>使用退化学习率对模型进行全局微调<br>1.加载本地模型<br>检查指定路径下是否存在模型文件，存在则加载本地模型，不存在则加载另一个路径下的模型。<br>2.微调模型<br>使用退化学习率对模型进行全局微调，通过调整学习率来优化模型性能，提高收敛速度和准确性。<br>3.训练和测试循环<br>对模型进行多次训练和测试，根据训练结果调整学习率，并保存最佳模型。<br>4.结果展示<br>在每个epoch结束后，打印当前的学习率，并在测试阶段打印模型的准确率等结果。</p><p>由于电脑配置问题暂时未跑满100轮深度学习，且由于本实验中样本比较小，且数据集鱼ImageNet数据集中的样本有重叠，因此训练的效果比较好。<br>已运行轮次的模型参数如下图所示：<br><img src="/images/1/1-1.png"><br>最终我们可以得到的：<br>每个训练阶段的损失和精度变化。<br>验证集上的总体准确率及各类别准确率。<br>模型在实际鸟类图像上的分类效果。</p><h4 id="11结论"><a href="#11结论" class="headerlink" title="11结论"></a>11结论</h4><p>模型性能<br>1.模型性能<br>通过系统训练和调整，我们的模型在测试集上展现了出色的性能。<br>2.准确率<br>模型的准确率逐渐提高，损失函数值持续下降，显示出良好的学习进度和优化效果。<br>3.微调策略<br>在最后的测试中，模型达到了接近80%的准确率，这证明了使用预训练模型并结合针对性的微调策略是有效的。</p><p>训练策略的有效性<br>1.模型训练两个阶段<br>第一阶段仅训练全连接层，快速适应新任务；第二阶段解冻所有层，进行全面训练。<br>2.分阶段训练策略<br>分阶段的训练策略能够加快初期学习速度，使模型在后期能够更深入地优化，适应复杂的鸟类图像特征。</p><p>迁移学习的优势<br>1.迁移学习<br>迁移学习可以显著减少从头开始训练模型所需的数据量和时间，通过利用在大规模数据集上学习到的丰富特征，我们可以将迁移学习应用于鸟类图像识别。<br>2.深度学习模型<br>深度学习模型具有很好的通用性和适应性，经过适当调整后，可以利用在大规模数据集上学习到的特征适用于鸟类图像的识别。</p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基于Pytorch实现猫狗分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using Neural Networks for Clothing Classification</title>
      <link href="/2023/10/30/GroupTwo/"/>
      <url>/2023/10/30/GroupTwo/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Using Neural Networks for Clothing Classification<br>利用神经网络进行服饰数据分类</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>利用神经系统进行数据服饰分类</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>高金  徐萌嘉</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="一、课程设计的目的"><a href="#一、课程设计的目的" class="headerlink" title="一、课程设计的目的"></a>一、课程设计的目的</h4><p>1.1本次课程训练选题</p><p>利用神经系统进行数据服饰分类</p><p>1.2选题的意义</p><p>神经系统能够模拟人类大脑的工作方式，具有强大的学习能力和智能识别能力，可以更准确地识别和分类数据，提高分类的准确性；神经系统在处理数据分类时能够并行处理大量信息，使得分类过程更加高效快速，节省时间和人力成本；神经系统对于复杂的数据模式和特征具有较强的适应能力，能够处理各种形式的数据，并且在面对大规模数据时也能保持良好的表现，可以实现实时数据分类，及时响应和处理不断变化的数据，满足对实时性要求较高的应用场景需求。</p><p>1.3选题对学习的帮助</p><p>本设计使用python软件编写，我们通过网络搜索等方式进行了软件的安装和环境的配置。利用神经系统进行服饰分类涉及大量数据的处理和分析，学习者在这个过程中可以提升学习者自己的数据处理能力和解决问题的能力，提升了学习者对整个项目的把控能力。</p><h4 id="二、需求分析"><a href="#二、需求分析" class="headerlink" title="二、需求分析"></a>二、需求分析</h4><p>2.1项目描述<br><img src="/images/2/1-1.png"><br>首先进行数据收集：收集大量的服饰图像数据作为训练集，包括不同种类、颜色、款式的服饰图片。</p><p>然后进行数据预处理：包括图像清洗、尺寸调整、标准化等操作，确保数据质量和一致性。<br>下一步构建神经网络模型：利用pytorch搭建四层全连接神经网络训练服饰分类模型。<br>接着模型训练：将预处理后的数据输入神经网络模型中进行训练，通过多次迭代学习服饰的特征和分类规律。</p><p>然后模型评估：使用测试集对训练好的模型进行评估，检查其在未见过的数据上的表现，调整模型参数以提高分类准确性。<br>最后模型部署：将训练好的模型部署到实际应用中，如服装电商平台，实现对服饰的自动分类和识别功能。</p><p>2.2实现功能</p><p>对服饰的自动分类和识别功能。</p><h4 id="三、实现步骤"><a href="#三、实现步骤" class="headerlink" title="三、实现步骤"></a>三、实现步骤</h4><p>3.1总括<br>通过业界知名的深度学习框架pytorch搭建四层全连接神经网络，分析Fashion-MNIST数据集中的六万张训练集图片和一万张测试集图片，观察训练误差和验证误差随训练代数提高的变化。<br><img src="/images/2/1-2.png"><br>3.2详细说明</p><p>首先构建包含多种服饰图片的数据集，接下来搭建并训练普通的四层全连接神经网络，训练后进行模型验证，观察效果，发现过拟合明显。 通过Dropout方法，减少过拟合。 你将掌握图像多分类、准确率与误差分析、搭建全连接神经网络并通过Adam算法进行梯度下降训练。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> torch  <span class="token comment"># 导入pytorch</span><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn<span class="token punctuation">,</span> optim  <span class="token comment"># 导入神经网络与优化器对应的类</span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms <span class="token comment">## 导入数据集与数据预处理的方法</span><span class="token comment"># 数据预处理：标准化图像数据，使得灰度数据在-1到+1之间</span>transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment"># 下载Fashion-MNIST训练集数据，并构建训练集数据载入器trainloader,每次从训练集中载入64张图片，每次载入都打乱顺序</span>trainset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span><span class="token string">'dataset/'</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>trainloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>trainset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment"># 下载Fashion-MNIST测试集数据，并构建测试集数据载入器trainloader,每次从测试集中载入64张图片，每次载入都打乱顺序</span>testset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span><span class="token string">'dataset/'</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transform<span class="token punctuation">)</span>testloader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>testset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="01-课题背景和意义"><a href="#01-课题背景和意义" class="headerlink" title="01.课题背景和意义"></a>01.课题背景和意义</h4><p>1.电子商务的发展<br>随着电子商务的快速发展和互联网的普及，人们购买服装的方式发生了巨大变化。<br>2.消费者购物困难<br>在在线购物中，消费者无法亲自接触和试穿服装，仅凭文字描述和图片进行选择存在一定的困难。<br>3.服装分类算法<br>开发一种准确、高效的服装分类算法系统，可以帮助消费者更好地了解和选择服装，提高用户体验和销售转化率。</p><h4 id="02-实现技术思路"><a href="#02-实现技术思路" class="headerlink" title="02.实现技术思路"></a>02.实现技术思路</h4><p>1.服装分类方法<br>深度学习：<br>深度学习方法的发展使得研究者开始关注网络宽度对性能的影响，并对网络结构进行优化。<br>注意力机制：<br>注意力机制在深度学习任务中被广泛应用，可以帮助网络模型快速定位关注的目标区域，从而提升准确率。<br>通道注意力：<br>SE-Attention是一种常用且被熟知的注意力机制形式，通过全局平均池化操作和可学习的权重生成通道级的全局特征。</p><p>ECA模块：<br>ECA模块相比SE-Attention模块，在进行全局平均池化操作后去除了全连接层，并使用一维卷积来实现跨通道信息交互。<br>NAM：<br>NAM通过抑制不重要的特征信息来提高学习效率，利用批标准化中的缩放因子来表示各个通道的变化情况，从而反映出每个通道的重要性。<br>权重稀疏性惩罚：<br>在注意力模块中引入权重稀疏性惩罚，以进一步抑制不重要特征信息，强调对于重要通道的注意力。</p><p>特征融合：<br>特征融合在卷积网络中通常可以分为早融合和晚融合两种方式，早融合指的是先将不同网络层提取的特征图进行融合。<br>经典融合方法：<br>两种经典的特征融合方法是Concat和Add，Concat方法直接将两个特征进行连接，输出特征的通道维度为两个输入特征的维度之和。<br>Add方法：<br>Add方法采用并行策略，将需要融合的两个特征向量组合成复合向量，通过元素相加的方式进行融合。</p><p>晚融合：<br>晚融合指的是将多个模型的预测结果进行相加，以得到最终的预测结果，可以选择不融合特征，而是对每个模型的预测分数进行综合分析。<br>金字塔结构：<br>晚融合也可以通过金字塔结构将不同尺度的特征图进行下采样或上采样操作来进行融合。<br>特征融合方式：<br>总的来说，特征融合可以采用早融合或晚融合方式，早融合通过Concat或Add方法将特征进行融合，晚融合通过综合分析预测分数或金字塔结构进行特征融合。</p><p>2.相关代码示例<br>DDSAConv模块：<br>为了更好地适应服装图像的整体形状特点，提出了一种新的模块称为DDSAConv。该模块基于EESP模块，并结合了基于归一化的通道注意力（NAM）。<br>归一化通道注意力：<br>DDSAConv模块通过一种新颖的分解卷积方式，在较少的计算量和参数量的情况下，有效地增加了网络对整体有效信息的学习能力。<br>分解卷积方式：<br>使用逐点群卷积可以使模型参数随着卷积核增加而减少，降低了过拟合的风险，并提供比标准卷积更好的模型。<br>群卷积应用：<br>结合了深度可分离卷积和空洞卷积，并设置不同的扩张率，使模型能够在相同参数下学习不同尺度的特征。<br>扩张率设置：<br>DDSAConv模块的结构包括逐点群卷积和四个具有不同扩张率的深度扩张可分离卷积，通过并行学习分组后的特征来捕捉服装图像的整体信息。<br>参数减少风险：<br>群卷积的应用可以减少模型的参数数量，从而有效降低过拟合的可能性，并提高模型的效率。</p><h4 id="03-数据集"><a href="#03-数据集" class="headerlink" title="03.数据集"></a>03.数据集</h4><p>1.数据集<br>自制数据集：<br>考虑到网络上缺乏合适的服装分类数据集，我决定自行制作。通过收集和拍摄各种服装照片，确保数据集包含丰富多样的服装样式、颜色和纹理特征。<br>真实场景拍摄：<br>我特别关注服装的款式、颜色、纹理等特征，并确保照片中能够清晰展示服装的细节。通过现场拍摄，我能够捕捉到真实的场景和多样的穿着环境。<br>数据集支持：<br>自制数据集能够为服装分类系统的性能提供有力支持。通过数据扩充方法，如镜像翻转、随机裁剪、旋转等，可以增加数据集的多样性和数量，改善模型的泛化能力。</p><p>2.数据扩充<br>数据扩充方法：<br>数据扩充方法如镜像翻转、随机裁剪、旋转、尺度变换、亮度对比度饱和度调整、噪声添加、颜色空间变换和数据混合等，可以增加数据集的多样性和数量。<br>改善泛化能力：<br>数据扩充方法可以根据具体需求选择和组合使用，通过自制数据集和数据扩充，可以为服装分类系统的研究和应用提供可靠的基础和有益的参考。</p><h4 id="04-实验及结果分析"><a href="#04-实验及结果分析" class="headerlink" title="04.实验及结果分析"></a>04.实验及结果分析</h4><p>1.实验环境搭建<br>①Python和PyTorch版本选择：<br>下载Python3.6.1和pytorch1.7.1，导入所需的神经网络和优化器类。<br>②数据集与预处理：<br>导入torchvision.datasets和transforms，进行数据预处理，标准化图像数据。<br>③训练集和测试集载入器：<br>构建训练集和测试集数据载入器，每次载入64张图片，并打乱顺序。</p><p>2.模型训练–搭建并训练四层全连接神经网络<br>改进卷积网络构建块：<br>通过改进卷积网络内部的构建块以及特征提取和注意力机制的优化，提高模型在服装分类任务上的性能。<br>特征提取与注意力机制：<br>在确定网络框架后，研究注意到原始结构中的SE注意力机制使用的降维方法不利于特征图中通道注意力的权重学习。<br>服装形状特征学习：<br>通过改进服装形状提取流的特征提取能力，包括减少网络层数、增加感受野的卷积核大小以及采用适合服装形状特征学习的注意机制方法，旨在提高网络在服装分类任务中的准确性。<br>特征融合与通道增强：<br>通过引入FFCE模块进行特征融合和通道增强，并对形状提取模块进行改进，PSCloNet在时尚图像分类任务中取得了显著的准确率提升。</p><p>3.相关代码示例<br>DDSAConv模块：<br>为了更好地适应服装图像的整体形状特点，提出了一种新的模块称为DDSAConv。<br>归一化通道注意力：<br>DDSAConv模块基于EESP模块，并结合了基于归一化的通道注意力（NAM）。<br>分解卷积与参数减少：<br>DDSAConv模块通过新颖的分解卷积方式，在减少计算量和参数量的情况下，增加了网络对整体有效信息的学习能力。</p><p>模型参数与过拟合风险：<br>使用逐点群卷积可以使模型参数随着卷积核增加而减少，降低了过拟合的风险，并提供比标准卷积更好的模型。<br>特征学习与扩张率：<br>结合深度可分离卷积和空洞卷积，并设置不同的扩张率，使模型能够在相同参数下学习不同尺度的特征。<br>群卷积与参数减少：<br>DDSAConv模块的结构包括逐点群卷积和四个具有不同扩张率的深度扩张可分离卷积，通过并行学习分组后的特征来捕捉服装图像的整体信息。<br>过拟合与模型效率：<br>群卷积的应用可以减少模型的参数数量，从而有效降低过拟合的可能性，并提高模型的效率。</p><p>4.验证模型效果<br>训练误差与测试误差：<br>训练误差持续下降，但测试误差仍高，显示网络可能过度关注训练数据，导致泛化能力不足。<br>泛化能力与预测概率：<br>网络有时能做出正确判断，但预测概率较低，表明其泛化能力有限，需要进一步改进。</p><p>5.采用dropout方法防止过拟合<br>Dropout正则化：<br>通过引入Dropout正则化，我们成功地避免了过拟合现象，使得训练误差和测试误差都随学习次数增加逐渐降低。<br>泛化能力的提升：<br>Dropout正则化通过防止神经网络过度关注训练数据，增强了其泛化能力，使其能够更好地处理新情境和未知数据。</p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 利用神经网络进行服饰数据分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bag of Multi-label Descriptors for Noisy Chest X-ray Classification</title>
      <link href="/2023/10/29/GroupThree/"/>
      <url>/2023/10/29/GroupThree/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Bag of Multi-label Descriptors for Noisy Chest X-ray Classification<br>胸部X射线噪声分类</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>BoMD通过利用来自BERT模型的词嵌入中包含的标签的语义信息来平滑地重新标记嘈杂的多标签医学图像数据集，用其邻域的估计标签分布重新标记有噪声的标签样本。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>王晓航 赵中哲</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><p>BoMD通过利用来自BERT模型的词嵌入中包含的标签的语义信息来平滑地重新标记嘈杂的多标签医学图像数据集，用其邻域的估计标签分布重新标记有噪声的标签样本。<br>BoMD有三个阶段: </p><ol><li>图像描述学习，将训练图像转换成一个视觉描述符包，这些描述符位于BERT语义空间中，由BERT模型从图像标签产生的词嵌入填充 ，学习一袋多标签图像描述符 (MID) 来表示一组BERT语义描述符的标签; </li><li>噪声标签样本检测基于验证从 (1) 的多个描述符估计的排名靠前的标签是否是该样本的多个标签，根据MID分类的标签排名识别噪声样本；<br>3)基于最近邻样本的标签传播，对噪声样本进行平滑重标记。这平滑地重新标记了数据集，然后将其用于训练多标签分类。使用通过MID构建的图的标签传播来平滑地重新标记多标签样本。<br>受BoW的激励，如下图MID通过将其多个标签与一个全局视觉描述符包关联来表示一个图像。MID使用一组视觉描述符将图像投影到BERT的语义空间中，这些描述符被优化，以提高它们与BERT模型产生的语义描述符的相似性，从图像的多标签注释。<br><img src="/images/3/1-1.png"><br>用学习到的图像描述符构建一个图，以便顺利地对训练数据重新贴标签。考虑到学习视觉描述符可能更接近清洁标签的语义空间，我们制定检测噪声训练样本第一排名（相似度降序）的标签图像（根据余弦相似度与单词嵌入标签）。<br>实验主要包含4个数据集, 即两个包含噪音的训练集NIH 和 ChestXpert以及两个干净的测试集OpenI, PadChest和NIH-Google. 实验的主要策略就是在NIH/ChestXpert上训练然后再OpenI/PadChest/NIH-Google上测试。<br><img src="/images/3/1-2.png"></li></ol><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h4 id="1-软件安装及使用"><a href="#1-软件安装及使用" class="headerlink" title="1.软件安装及使用"></a>1.软件安装及使用</h4><p>优势<br>由于Python的开源性，其中涉及到版本兼容问题，还要根据源码环境做环境配置调试。<br>教程<br>·<a href="https://blog.csdn.net/qq_32892383/article/details/116137730.%EF%BC%88%E5%9B%BE%E6%96%87%EF%BC%89">https://blog.csdn.net/qq_32892383/article/details/116137730.（图文）</a><br>·<a href="https://space.bilibili.com/456667721%EF%BC%88%E8%A7%86%E9%A2%91%EF%BC%89">https://space.bilibili.com/456667721（视频）</a></p><h4 id="2-源码、数据下载，环境配置"><a href="#2-源码、数据下载，环境配置" class="headerlink" title="2.源码、数据下载，环境配置"></a>2.源码、数据下载，环境配置</h4><p>原始数据<br><img src="/images/3/2-1.png"><br>已处理的数据及环境配置<br><img src="/images/3/2-2.png"></p><h4 id="3-BOMD"><a href="#3-BOMD" class="headerlink" title="3.BOMD"></a>3.BOMD</h4><p>3.1 利用估计的来自其邻域的标签分布重新标记含噪多标签图像<br><img src="/images/3/2-3.png"><br>所提出的BoMD有两个阶段1 )图像描述学习，将训练图像转化为视觉描述符袋，该视觉描述符袋位于由图像标签计算的词嵌入填充的语义空间中；<br>2 )图构建，平滑地重新标记含噪多标签图像，其中每幅图像由从学习袋构建的子图表示</p><p>3.2 将图像的多个标签关联到一袋全局视觉描述符来表示图像<br><img src="/images/3/2-4.png"><br>MID使用一组经过优化的视觉描述符将图像投影到BERT的语义空间中，以促进它们与BERT模型从图像的多标签标注中产生的语义描述符的相似性。</p><p>3.3<br><img src="/images/3/2-5.png"><br>01.<br><img src="/images/3/2-6.png"><br>02.<br><img src="/images/3/2-7.png"><br>AUC：ROC曲线与坐标轴围成的面积，衡量学习器的性能。<br>mAP:多类别的平均精度的平均值，主要是用来衡量分类器的效果。</p><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="项目问题和解决方法"><a href="#项目问题和解决方法" class="headerlink" title="项目问题和解决方法"></a>项目问题和解决方法</h4><p>1、项目所需要工具库？<br>由于在论文和readme中，并没有提及到所使用的工具库，所以无法提前安装。所以我们采用边跑边安装库的方法。从目前来看，整个项目所用到的库：faiss、faiss-gpu（均为1.7.4版本）、gsutil、wandb、loguru等，工具库的安装较为繁琐，而且没有使用库的说明，需要的时间较长。</p><p>2、数据库的下载问题？<br>项目所需要的数据库是基于美国卫生局所整合的病人的胸片图片，数据量极为繁重，同时在下载数据库的时候需要爬墙，数据库的数据已经做过处理并不需要修饰。从项目来看，所需要的数据库有NIH、open-i、CXR数据库等，主要是CXR数据集。</p><p>3、环境的搭建问题？<br>整个项目分为三个部分两个训练集，一个测试集，整体环境参数已经给定，但是由于设备的问题，需要调整batch-size，原本的batch-size为16，调整后为8，后又经过调整为1，训练集和测试集的环境变量均为pythonunbuffered=1,cuda-visible-devices=0。</p><p>4、ValueError: Length of values (44838) does not match length of index (22419)<br><img src="/images/3/3-1.png"></p><h4 id="源码、数据下载，环境配置"><a href="#源码、数据下载，环境配置" class="headerlink" title="源码、数据下载，环境配置"></a>源码、数据下载，环境配置</h4><p>3.1 利用估计的来自其邻域的标签分布重新标记含噪多标签图像<br>所提出的BoMD有两个阶段1 )图像描述学习，将训练图像转化为视觉描述符袋，该视觉描述符袋位于由图像标签计算的词嵌入填充的语义空间中；<br>2 )图构建，平滑地重新标记含噪多标签图像，其中每幅图像由从学习袋构建的子图表示<br><img src="/images/3/3-2.png"></p><p>3.2 将图像的多个标签关联到一袋全局视觉描述符来表示图像<br>MID使用一组经过优化的视觉描述符将图像投影到BERT的语义空间中，以促进它们与BERT模型从图像的多标签标注中产生的语义描述符的相似性。<br><img src="/images/3/3-3.png"></p><p>3.3<br><img src="/images/3/3-4.png" alt="01"><br><img src="/images/3/3-5.png" alt="02"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 胸部X射线噪声分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Deep Reinforcement Learning Approach to Supply Chain Inventory Management</title>
      <link href="/2023/10/28/GroupFour/"/>
      <url>/2023/10/28/GroupFour/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>A Deep Reinforcement Learning Approach to Supply Chain Inventory Management<br>供应链库存管理的深度强化学习方法</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>利用强化学习和深度学习的最新发展来解决供应链库存管理(SCIM)问题</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>王海栋 刘明培 全晓杰 李尧</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="概要"><a href="#概要" class="headerlink" title="概要"></a>概要</h4><p>供应链和价格管理是企业运营中最早采用数据科学和组合优化方法的领域之一。深度强化学习（深度学习加强化学习）定价有可能大大提高这些和其他类型的企业运营的优化能力。<br>我们利用强化学习和深度学习的最新发展来解决供应链库存管理(SCIM)问题，这是一个复杂的顺序决策问题，包括确定在给定时间范围内生产和运送到不同仓库的产品的最佳数量。给出了随机两梯队供应链环境的数学表达式，该环境允许管理任意数量的仓库和产品类型。</p><h4 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h4><p>我们（论文中）设计了一个发散的两级供应链，包括一个工厂、一个工厂仓库和J个配送仓库;这种结构的一个例子如图所示。<br><img src="/images/4/1-1.png" alt="工厂、仓储仓库、运输、商店、购买者"><br>具体要考虑的参数有：产品随时间季度变化的收入，生产成本，运输成本，总体存储成本，总利润等。</p><h4 id="所需具体软件"><a href="#所需具体软件" class="headerlink" title="所需具体软件"></a>所需具体软件</h4><p><img src="/images/4/1-2.png"><br><img src="/images/4/1-3.png"></p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 供应链库存管理的深度强化学习方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CLIP</title>
      <link href="/2023/10/27/GroupFive/"/>
      <url>/2023/10/27/GroupFive/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>CLIP<br>CLIP多模态模型</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>图像分类和检索、内容调节</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>张宗一 王皓</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="一：CLIP多模态是什么"><a href="#一：CLIP多模态是什么" class="headerlink" title="一：CLIP多模态是什么"></a>一：CLIP多模态是什么</h4><p>CLIP是一种多模态预训练模型，用于将文本和图像信息相结合，实现强大的跨模态理解和生成能力。</p><p>CLIP模型由OpenAI于2021年推出，其核心思想是将视觉和语言的表示方式相互联系起来，从而实现图像分类任务。CLIP模型采用了对比学习(Contrastive Learning)和预训练(Pre-Training)的方法，使得模型能够在大规模无标注数据上进行训练，并学习到具有良好泛化能力的特征表示。在CLIP模型中，图像和文本被映射到同一表示空间，并通过对比不同图像和文本对之间的相似性和差异性进行训练，从而学习到具有区分度的特征表示。<br>Clip（Contrastive Language-Image Pre-Training）是由OpenAI于2021年推出的一种深度学习模型，它是一种可以同时处理文本和图像的预训练模型。与以往的图像分类模型不同，Clip并没有使用大规模的标注图像数据集来进行训练，而是通过自监督学习的方式从未标注的图像和文本数据中进行预训练，使得模型能够理解图像和文本之间的语义联系。</p><p>CLIP(Contrastive Language Image Pretraining)这篇文章出自OPEN-AI大名鼎鼎的Alec-Radford（GPT系列的一作，在GAN，Diffusion等各种生成领域都颇有影响力）。而CLIP这篇论文可以看做是多模态在预训练时代的一次妙到巅峰的任务设计。<br>NLP领域里借助海量文本进行无（自）监督式的预训练使得各种与下游任务类型无关的模型架构成为可能，并取得了非常好的迁移性和效果。CLIP使用了一种对比学习的方式，在4亿图文对上进行了文本和图片的匹配任务训练，使得该模型在无任何微调的情况下（zero-shot），在imageNet上取得了和ResNet-<br><img src="/images/5/1-1.png"><br>50微调后一样的效果。</p><p>Clip模型的核心思想是通过学习图像和文本之间的匹配关系来提高模型的性能。具体来说，Clip模型包含两个主要组成部分：一个用于处理图像的卷积神经网络（CNN）和一个用于处理文本的Transformer模型。这两个组件都被训练成能够将输入的信息映射到相同的嵌入空间中，并使得相似的图像和文本在嵌入空间中的距离更近。</p><p>Clip模型的预训练分为两个阶段：第一阶段是通过一个大规模的文本数据集来训练Transformer模型，使得模型能够理解文本之间的关系；第二阶段则是使用一个大规模的图像和文本数据集来训练整个Clip模型，使得模型能够将文本和图像之间的联系进行匹配。实现的伪代码如下：<br><img src="/images/5/1-2.png"></p><h4 id="二：所用的软件"><a href="#二：所用的软件" class="headerlink" title="二：所用的软件"></a>二：所用的软件</h4><ol><li>python<img src="/images/5/1-3.png"></li><li>anaconda<img src="/images/5/1-4.png"></li><li>cuda<img src="/images/5/1-5.png"></li><li>pycharm<img src="/images/5/1-6.png"></li></ol><h4 id="三：我们组所做的内容"><a href="#三：我们组所做的内容" class="headerlink" title="三：我们组所做的内容"></a>三：我们组所做的内容</h4><p>图像分类和检索：CLIP可以通过将图像与自然语言文本描述关联起来进而可用于图像分类任务。它允许更通用和灵活的图像检索系统，用户可以使用文本查询来在数据库里搜索图像。<br>内容调节：CLIP可用于通过分析图像和附带文本来识别和过滤不适当或有害的内容，从而调节在线平台上的展示内容。</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="从零实现CLIP模型"><a href="#从零实现CLIP模型" class="headerlink" title="从零实现CLIP模型"></a>从零实现CLIP模型</h4><p>CLIP代表语言图像对比预训练模型，是OpenAI于2021年开发的一个深度学习模型。CLIP模型中图像和文本嵌入共享相同的潜在特征空间，从而能够在两种模式之间直接进行对比学习。这是通过训练模型使相关的图像和文本更紧密地结合在一起，同时将不相关的图像在特征空间距离分开来实现的。</p><p>关于CLIP模型的一些应用总结如下： 图像分类和检索：CLIP可以通过将图像与自然语言文本描述关联起来进而可用于图像分类任务。它允许更通用和灵活的图像检索系统，用户可以使用文本查询来在数据库里搜索图像。 内容调节：CLIP可用于通过分析图像和附带文本来识别和过滤不适当或有害的内容，从而调节在线平台上的展示内容。</p><h4 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h4><p>CLIP模型旨在预测一个batch中N×N个潜在(img,text)配对具体哪些是实际匹配的。为了实现这一点，CLIP通过图像编码器和文本编码器的联合训练建立了一个多模态嵌入空间。CLIP的损失函数旨在最大化批处理中N个真实配对的图像和文本嵌入之间的余弦相似性，同时最小化N²−N个错误配对的余弦相似度。<br><img src="/images/5/3-1.png"></p><h4 id="clip模型具体的网络结构"><a href="#clip模型具体的网络结构" class="headerlink" title="clip模型具体的网络结构"></a>clip模型具体的网络结构</h4><p><img src="/images/5/3-2.png"></p><h4 id="数据输入"><a href="#数据输入" class="headerlink" title="数据输入"></a>数据输入</h4><p>该模型每个批次以 n 个图像和文本对作为输入，其中：I[n，h，w，c]：表示对齐的图像的小批次输入，其中n是batch大小，h是图像高度，w是图像宽度，c是通道数。 T[n，l]：表示对齐文本的小批次输入，其中n是batch大小，l是文本序列的长度。我们的实现中，我们默认batch的大小为128，如下所示：<br><img src="/images/5/3-3.png"></p><h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><p>关于文本和图像的特征提取，这里使用resnet34和distilbert来分别提取图像和文本的特征，如下： I_f = image_encoder(I) : 从图像编码器中获取的图像特征表示I_f。I_f的大小为[n，d_I]，其中d_I是图像特征的维度。 T_f=text_encoder(T)：从文本编码器中获取的文本特征表示T_f。T_f的大小为[n，d_T]，其中d_T是文本特征的维度。</p><pre class="line-numbers language-none"><code class="language-none"># for encoding imagesI_f = models.resnet34(pretrained=True)     # for encoding captionsT_f= AutoModel.from_pretrained(“distilbert-base-multilingual-cased”)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>接着，我们将相应的文本和图像特征，映射到同一嵌入特征空间，如下： W_i[d_i,d_e]：表示用于将图像特征i_f映射到嵌入特征空间i_e的投影矩阵。W_i的形状大小是[d_i，d_e]，其中d_e表示的是联合嵌入特征空间的维度。W_t[d_t,d_e]：表示用于将文本特征t_f映射到相同嵌入空间t_e的投影矩阵。W_t的形状大小是[d_t，d_e]。投影操作可以使用具有两个线性层的神经网络进行编码，其权重是学习的投影矩阵。在大多数情况下，投影权重是唯一可以在新数据集上需要训练的权重。此外，投影层在对齐图像和文本嵌入的尺寸方面发挥着至关重要的作用，确保它们具有相同的维度。<br><img src="/images/5/3-4.png"><br>接着我们将上述相关组件进行整合： I_e  = l2_normalize(np.dot(I_f, W_i), axis=1) ：在联合嵌入空间I_e中嵌入并归一化图像特征 T_e = l2_normalize(np.dot(T_f, W_t), axis=1) ：在联合嵌入空间T_e中嵌入并归一化文本特征接着我们使用以下Pytorch代码来描述图像和文本数据的处理次序。首先，相应的数据通过基本编码器进行处理，然后通过投影层进行处理。最后，为两种模态特征进行嵌入归一化化并返回。<br><img src="/images/5/3-5.png"><br>接着在嵌入空间，我们来计算文本图像嵌入特征的相似度：logits = np.dot(I_e, T_e.T) * np.exp(t)：用以计算图像和文本对在联合嵌入空间的特征余弦相似度，通过可学习的参数 t 进行缩放。在我们的例子中，我们考虑暂不使用参数 t，代码如下：</p><pre class="line-numbers language-none"><code class="language-none">logits = T_e @ T_e.T<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>CLIP使用对比损失用以将相关图像和文本在嵌入特征空间拉近，同时将不相关的图像和文本距离拉远。labels = np.arange(n): 用以生成表示batch索引的真值标签。loss_i = cross_entropy_loss(logits, labels, axis=0)：用以计算图像特征和真值标签的损失loss_t = cross_entropy_loss(logits, labels, axis=1)：用以计算文本特征和真值标签的损失 loss = (loss_i + loss_t)/2：计算图像和文本损失的加权平均值。<br><img src="/images/5/3-6.png"></p><h4 id="构建完整模型"><a href="#构建完整模型" class="headerlink" title="构建完整模型"></a>构建完整模型</h4><p><img src="/images/5/3-7.png"><br>我们的自定义CLIP模型将使用flickr30k数据集进行训练。该数据集包括31000多张图像，每张图像至少有5个独立的人工生成文本描述。在这个例子中，我们将为每个图像使用两个标题，总共有62000个图像和文本对用于训练。<br><img src="/images/5/3-8.png"><br>上述模型关键常数包括用于学习表示特征空间的维度embed_dim, 用于transformer特征维度的transformer_embed_dim和用于文本输入长度的max_len。所选的text_model是distilbert base multilanguage-cased。用以训练的模型的epoch为3，同时batch_size的大小为128，这些常数将用于模型构建和训练<br><img src="/images/5/3-9.png"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CLIP多模态模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QLORA Efficient Finetuning of Quantized LLMs</title>
      <link href="/2023/10/26/GroupSix/"/>
      <url>/2023/10/26/GroupSix/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>QLORA: Efficient Finetuning of Quantized LLMs<br>大模型微调</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>基于大模型的内在低秩特性，增加旁路矩阵来模拟全参数微调，LoRA 通过简单有效的方案来达成轻量微调的目的。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>张义 郭俊桐</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="Adapter-Tuning"><a href="#Adapter-Tuning" class="headerlink" title="Adapter Tuning"></a>Adapter Tuning</h4><p>通过添加Adapter模块来避免全模型微调与灾难性遗忘的问题。Adapter方法不需要微调预训练模型的全部参数，通过引入少量针对特定任务的参数，来存储有关该任务的知识，降低对模型微调的算力要求。</p><h4 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix Tuning"></a>Prefix Tuning</h4><p>前缀微调（prefix-tunning），用于生成任务的轻量微调。前缀微调将一个连续的特定于任务的向量序列添加到输入，称之为前缀，如下图中的红色块所示。与提示（prompt）不同的是，前缀完全由自由参数组成，与真正的token不对应。相比于传统的微调，前缀微调只优化了前缀。因此，我们只需要存储一个大型Transformer和已知任务特定前缀的副本，对每个额外任务产生非常小的开销。</p><h4 id="P-Tuning"><a href="#P-Tuning" class="headerlink" title="P-Tuning"></a>P-Tuning</h4><p>同时加了两个改动： 1. 考虑到预训练模型本身的embedding就比较离散了（随机初始化+梯度传回来小，最后只是小范围优化），同时prompt本身也是互相关联的，所以作者先用LSTM对prompt进行编码 2. 在输入上加入了anchor，比如对于RTE任务，加上一个问号变成[PRE][prompt tokens][HYP]?[prompt tokens][MASK]后效果会更好</p><h4 id="Prompt-Tuning"><a href="#Prompt-Tuning" class="headerlink" title="Prompt Tuning"></a>Prompt Tuning</h4><p>Prompt-tuning给每个任务定义了自己的Prompt，拼接到数据上作为输入，同时freeze预训练模型进行训练，在没有加额外层的情况下，随着模型体积增大效果越来越好。</p><h4 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h4><p>基于大模型的内在低秩特性，增加旁路矩阵来模拟全参数微调，LoRA 通过简单有效的方案来达成轻量微调的目的。它的应用自不必提，可以将现在的各种大模型通过轻量微调变成各个不同领域的专业模型。</p><p>此外，考虑 OpenAI 对 GPT 模型的认知，GPT的本质是对训练数据的有效压缩，从而发现数据内部的逻辑与联系，LoRA 的思想与之有相通之处，原模型虽大，但起核心作用的参数是低秩的，通过增加旁路，达到四两拨千斤的效果。QLoRA就是对模型进行量化和压缩，可以在消费级显卡上进行大模型的微调。</p><p>我们接下来的目标就是通过有限的算力进行大模型微调，引用《QLORA: Efficient Finetuning of Quantized LLMs》这篇论文，采用其中作者发布到github上的开源代码，对Llama2模型进行微调。</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>本文介绍了大模型微调在文本分类任务中的应用。我们首先解释了大模型微调的概念和原理，然后提供了一个Python代码示例，演示了如何使用预训练的大型语言模型BERT进行微调，以进行文本分类任务。通过实验和结果分析，我们展示了大模型微调在文本分类任务中的有效性和优势性，首先我们先对大模型微调有一个了解。<br>大模型微调是指在一个预先训练好的大型神经网络模型上，通过少量的数据和少量的训练轮次来调整模型以适应特定任务或领域。大模型微调通常用于迁移学习，能够在相对较小的数据集上取得比从头训练模型更好的效果。<br>我的看法是，大模型微调是一种非常有效的方法，特别适合在数据稀缺的情况下使用。通过在预训练的大模型上进行微调，可以利用模型在大规模数据上学到的通用特征和知识，从而更快速地学习适应新任务的特定特征。这种方法不仅可以提高模型的泛化能力，还可以节省时间和计算资源。<br>然而，在实际应用中，需要注意的是选择合适的预训练模型、调整学习率、冻结部分层以及调整超参数等微调技巧。此外，还需要谨慎选择微调数据集，确保数据集与目标任务相关性高，以充分发挥微调的效果。<br>总的来说，大模型微调是一个非常值得尝试的方法，可以帮助提升模型的性能和泛化能力，尤其在数据稀缺的情况下具有很大的优势。<br><img src="/images/6/3-1.png"><br><img src="/images/6/3-2.png"></p><h4 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h4><p>通过实验我们发现，利用BERT进行文本分类任务的微调，相比于从零训练模型，可以获得更好的性能和更快的收敛速度。大模型微调在文本分类任务中展现出了显著的优势。</p><h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>本文重点介绍了大模型微调在文本分类任务中的应用，并通过实验和结果分析验证了其有效性和优势。未来可以进一步探索不同领域和任务中大模型微调的应用。<br><img src="/images/6/3-3.png" alt="第一步导入相关包"><br><img src="/images/6/3-4.png" alt="第二步加载数据集"><br><img src="/images/6/3-5.png" alt="第三步数据集预训练"><br><img src="/images/6/3-6.png" alt="第四步创建模型"><br><img src="/images/6/3-7.png" alt="训练结果"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大模型微调 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Predicting Human Scanpaths in Visual Question Answering</title>
      <link href="/2023/10/25/GroupSeven/"/>
      <url>/2023/10/25/GroupSeven/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Predicting Human Scanpaths in Visual Question Answering<br>在视觉问答中预测人类的视觉扫描路径</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>利用深度学习方法预测在视觉问答（VQA）中不同的扫描路径。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>孟令琦 肖博雅 董一</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="1-干什么"><a href="#1-干什么" class="headerlink" title="1.干什么"></a>1.干什么</h4><p>利用深度学习方法预测在视觉问答（VQA）中不同的扫描路径。<br>在有任务引导时，生成的扫描路径有所差异。<br>此模型预测视觉问答中人类行为的时空模型，如路径、持续时间和顺序，并推广到自由观看和视觉搜索任务。</p><h4 id="2-深度学习模型"><a href="#2-深度学习模型" class="headerlink" title="2.深度学习模型"></a>2.深度学习模型</h4><p>CovnLSTM（长短时记忆网络模型）</p><h4 id="3-训练方法"><a href="#3-训练方法" class="headerlink" title="3.训练方法"></a>3.训练方法</h4><p>利用正确和错误两种不同的扫描路径进行训练。</p><h4 id="4-流程"><a href="#4-流程" class="headerlink" title="4.流程"></a>4.流程</h4><p><img src="/images/7/1-1.png"></p><h4 id="5-达到效果"><a href="#5-达到效果" class="headerlink" title="5.达到效果"></a>5.达到效果</h4><p>输入图片后，输出预测的人类对该图片观察的路径、顺序和持续时间。</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="相关理论和技术"><a href="#相关理论和技术" class="headerlink" title="相关理论和技术"></a>相关理论和技术</h4><p>1.视觉问答（VQA）<br>     视觉问答（Visual Question Answering,VQA）是一项旨在让计算机能够回答与图像相关的自然语言问题的重要任务。在VQA任务中，计算机需要学会识别图像中的物体和场景，并理解问题的语义和结构，然后才能够正确回答问题。因此，VQA任务需要模型能够融合不同领域的知识，并且要深入探究图像和文本之间的语义关系。我们主要运用VQA模型来作为任务指导。<br><img src="/images/7/3-1.png"></p><p>2.COCO-search18数据集<br>      COCO-Search18是第一个足够大的实验室质量目标导向行为并且可以训练深度网络模型的数据集。它收集了10个人在6202张自然场景图像中搜索18个目标对象类别的眼球运动行为，产生了约30万个搜索注视点。<br><img src="/images/7/3-2.png"><br>A图：黄线和编号表示着来自单个参与者的眼球扫描路径。从左到右，从上到下：瓶子、碗、汽车、椅子、模拟时钟、杯子、叉子、键盘、刀、笔记本电脑、微波炉、鼠标、烤箱等。<br>B图：对相同场景的参与者计算的注视密度图示例</p><p>3.人类路径扫描任务<br>    人眼扫视路径（saccadic scanpath）的过程，是人类视觉系统在短暂时间内迅速完成图像浏览任务的过程，具体包括注视（fixation）和扫视（saccade）两种行为。<br>       给定一幅图像或者视频，扫视路径预测的任务就是预测一条连续的时空轨迹，通过图像空间坐标以及注视时长构成的三维点序列，如图(c)所示。图(b)为通过采集所有受测者的眼动点数据进行高斯平滑和归一化得到的显著性概率模拟图，图(c)在图(a)只显示一位受测者扫视路径，叠加了图(b)的热图层，圆圈中心为注视点位置，半径为比例缩放后的注视时长，数字为注视点在扫视路径上的顺序。<br><img src="/images/7/3-3.png"></p><p>4.注意力机制<br>  随着深度神经网络的快速发展，注意机制已成为提高VQA模型的性能和可解释性的重要组成部分。然而，由于它们的内在差异，机器注意在许多情况下与人类的注意不一致。为了研究人类注意和机器注意之间的关系，Chen等人开发了数据集和计算方法来对人类和VQA模型的注意图进行测量、建模和比较分析。<br>  虽然这些分析关注的是正确答案和错误答案之间的注意力空间差异，但在本次研究中产生了个体的注视来研究人们如何保持和转移注意力，这也编码了时间信息，如持续时间和顺序。通过对机器注意的显式结合，研究还提供了一种替代方法来衡量VQA模型在引导扫描路径预测方面的有效性。<br>5.CNN、ConvLSTM、反向传播算法<br>        卷积神经网络（CNN）的基本结构由输入层、卷积层 (convolutional layer) 、池化层 (pooling layer, 也称为取样层) 、全连接层及输出层构成.卷积层和池化层一般会取若干个, 采用卷积层和池化层交替设置, 即一个卷积层连接一个池化层, 池化层后再连接一个卷积层, 依此类推，由于卷积层中输出特征面的每个神经元与其输入进行局部连接, 并通过对应的连接权值与局部输入进行加权求和再加上偏置值, 得到该神经元输入值。<br><img src="/images/7/3-4.png"><br>        ConvLSTM主要由输入门，遗忘门，输出门和卷积层组成。在输入门中，ConvLSTM使用卷积层来计算当前时间步的输入与上一个时间步的隐藏层的相关性，并用来控制当前时间步的输入信息；在遗忘门中，ConvLSTM使用卷积层来计算当前时间步的输入和上一时间步的隐藏层的相关性，用来控制遗忘上一时间步的隐藏层信息；在输出门中，ConvLSTM使用卷积层来计算当前时间步的输入和上一时间步的隐藏层的相关性，并用来输出当前时间步的隐藏层信息这样ConvLSTM就可以在处理时间序列上的图像数据时，同时考虑空间和时间的特征。<br><img src="/images/7/3-5.png"><br>         反向传播算法，是目前实现神经网络模型训练过程最常用的优化算法，其理论基础为梯度下降法。具体来说，在网络的迭代训练过程中，不断求取网络输出结果的误差，利用该误差对各层参数求取梯度，使各层参数的朝着梯度最大的负方向进行迭代优化，可以保证模型参数收敛，直至最终的模型输出误差最小化。这个过程包括前向计算，误差反向传播，以及梯度更新三个步骤。</p><p>6.哈达玛积<br>          哈达玛积是矩阵的一类运算，若A=（aij）和B=（bij）是两个同阶矩阵，若cij=aij×bij，则称矩阵C=（cij）为A和B的哈达玛积，或称基本积，记作A○B。<br>        因为哈达玛矩阵的元素只有1和-1，所以卷积运算结果可以使图像的高频部分被抑制，低频部分增强或者使图像中的一些特定部分被增强，从而实现图像的平滑处理、锐化处理或者特征提取。<br><img src="/images/7/3-6.png"><br>7.SCST与CDL（新的一致性发散损失）<br>     SCST（自我关键序列训练）是流行的加强算法的一种形式，它不是估计基线来归一化奖励和减少方差，而是利用自己的测试时间推理算法的输出来归一化它所经历的奖励。通过直接优化不可微的测试时间度量来解决这种偏差。利用SCST的有效性，我们进一步引入了新的一致性-散度损失来学习正确和不正确扫描路径之间的差异。使用这种方法，可以避免估计奖励信号和估计归一化，同时使模型与其测试时间推理过程相协调。<br>       具体来说，给定正确和不正确的真实扫描路径，我们首先计算它们的组内相似度r*+、r*-，以及组间相似度r∗，通过平均正确和不正确组内部和之间的两两评估分数,比较∆r*+和∆r*-。直观地看，高组内相似性和低组间相似性表明，正确和不正确的扫描路径之间的差异更容易区分。<br><img src="/images/7/3-7.png"></p><h4 id="研究流程"><a href="#研究流程" class="headerlink" title="研究流程"></a>研究流程</h4><p>1.扫描路径预测模型<br><img src="/images/7/3-8.png"><br>2.SCST<br>（Self-critical Sequence Training for Image Captioning）<br>SCST强化学习<br>监督学习<br>利用自己的测试时间推理算法的输出来归一 化它所经历的奖励。通过直接优化不可微的测试时间度量来解决这种偏差。利用 SCST 的有效性，我们进一步引入了一致性-散度损失来学习正确和不正确扫描路 径之间的差异。使用这种方法，可以避免估计奖励信号和估计归一化，同时使模 型与其测试时间推理过程相协调。</p><h4 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h4><p>在训练train部分中，总共分为三部分：常规训练、强化训练、验证评估<br><img src="/images/7/3-9.png"><br><img src="/images/7/3-10.png"></p><p>传统训练阶段<br>常规训练分为四个步骤：数据加载、前向传播、损失计算、反向传播、参数更新。<br>定义名为train的函数。判断当前训练轮数是否小于强化训练轮数，如果是则执行传统训练阶段。<br>从数据加载器中提取出图像、扫描路径、提取时间。每次迭代开始之前都要清零优化器中的梯度，若指定消融注意力信息，则attention-map（注意力映射）为0</p><p>将图像、注意力映射、任务作为输入，通过模型前向传输，得到预测结果<br>总损失的计算公式：总损失=动作预测损失+args.lambda_1 *持续时间损失<br>反向传播：计算损失相对于模型参数的梯度，若有阈值，则进行梯度裁剪<br>使用优化器更新模型参数，迭代次数，学习率。记录到tensorboard中以达到可视化。</p><p>强化学习阶段<br>开启评估模式 。在PyTorch中通常用于关闭dropout层和batch normalization层的某些特性，确保模型在评估或测试时的行为与训练时一致。准备相关数据代码，遍历强化模型加载器，进行模型前向传播。<br>通过随机采样和模拟来评估模型在强化学习中的性能<br>初始化列表，使用模型对给定的images, attention_maps, 和 tasks进行预测。预测结果包括对数正态分布的均值（log_normal_mu）、方差（log_normal_sigma2）以及所有可能动作概率<br>随机采样选择的动作概率、持续时间和实际选择动作。生成模拟的扫描路径，评估模拟的扫描路径与真实的固定向量（gt_fix_vectors）之间的匹配度。这里使用了两个ScanMatch对象，一个考虑持续时间，另一个不考虑。检查并处理评估结果，循环终止。</p><p>使用了均值作为奖励信号，并根据这个奖励信号来更新模型的参数。<br>将metrics_reward_batch中的所有奖励张量拼接在一起，形成一个更大的张量。计算每个样本在所有时间步上的奖励均值。将奖励均值转化为张量并移到GPU。分别计算与动作相关和与持续时间相关的负对数似然损失，再得到总损失。计算损失对于模型参数的梯度，并用阈值限制以后防止梯度爆炸，使用优化器来更新模型参数。再增加迭代次数。将训练过程中的损失、奖励、学习率以及多匹配指标记录到TensorBoard中。</p><p>验证过程<br>用预训练的模型（model）来预测验证数据集上的结果，并将这些预测结果与真实的目标值（gt_fix_vectors）进行比较，以计算性能指标。<br>初始化以后开始遍历验证数据集：数据预处理，注意力图处理，模型预测，随机采样并生成预测固定向量。随后会计算真实目标值和预测值之间的性能指标，打印评估指标<br>在训练过程中评估模型的性能，包括与人类性能的基准比较，并在每个训练周期后计算特定指标的调和平均数。记录度量值、保存模型检查点、记录日志以及在特定条件下保存监督训练文件。</p><p>test阶段，一共分为以下五个部分<br><img src="/images/7/3-11.png"></p><p> 将程序运行在验证模式下，定义了以下参数。<br>–img_dir：图像数据的目录，默认是”./data/images”。<br>–fix_dir：原始注视点文件（fixation files）的目录，默认是”./data/fixations”。<br>–detector_dir：显著性映射（saliency maps）的目录，默认是”./data/detectors”。<br>–width 和 –height：输入数据的宽度和高度，默认分别是320和240。<br>–map_width 和 –map_height：输出数据的宽度和高度，默认分别是40和30。<br>–batch：批处理大小，默认是16。<br>–seed：随机种子，默认是0。<br>–detector_threshold：检测器的阈值，默认是0.8。<br>–gpu_ids：使用的GPU的ID列表，默认是[0, 1]，表示使用第一个和第二个GPU。<br>–evaluation_dir：从哪个特定目录恢复模型，默认是”./assets/pretrained_model”。<br>–eval_repeat_num：评估的重复次数，默认是10。<br>–min_length 和 –max_length：生成扫描路径的最小和最大长度，默认分别是1和16。<br>–ablate_attention_info：一个布尔值，表示是否进行注意力信息的消融，默认是False。<br>&nbsp;<br>自定义一个数据集<br>配置日志记录，加载数据集参数，创建数据加载器，初始化模型，初始化采样器。数据集实例使用了从命令行参数中获取的多个参数，包括图像目录、注视点目录、显著性映射目录等。<br>数据加载器中设置了批处理大小（args.batch）、是否打乱数据（shuffle=False，因为验证时通常不需要打乱数据）、工作进程数（num_workers=4）以及一个自定义的 collate_fn（通常用于自定义批次数据的处理方式）。</p><p>加载检查点，并设置在多个GPU中，再进行人类性能评估<br>这个检查点文件通常包含了模型的状态字典（state_dict），可能还包括优化器的状态和其他相关信息。通过遍历 validation_checkpoint 的键，除了 “optimizer” 键之外，其他的键对应的值都用来加载模型的 state_dict。strict=False 参数意味着如果模型的状态字典中的某些键不在加载的检查点中，加载过程不会抛出错误。<br>human_evaluation 函数返回三个值：human_metrics（人类性能的度量指标），human_metrics_std（这些度量指标的标准差），以及gt_scores_of_each_images（每幅图像的真实分数）<br> 遍历 human_metrics 的每个键（可能是不同的度量指标，如准确率、召回率等），然后对于每个键，再遍历其对应的值（可能是不同条件下的度量值）。<br>使用字符串格式化来生成日志信息，包括度量指标的名称、具体条件以及对应的值和标准差。这些信息将被写入之前设置的日志文件。</p><p>对模型进行评估，使用模型对验证数据集进行预测，并收集相关的预测结果和真实值<br>repeat_num = args.eval_repeat_num：从命令行参数中获取验证过程的重复次数。<br>all_gt_fix_vectors 和 all_predict_fix_vectors：这两个列表用于存储真实（ground truth）和预测的注视点向量。<br>predict_results：用来存储预测的结果<br>使用 tqdm 库创建一个进度条 pbar_val，用于在验证过程中显示进度。总进度条的长度是数据加载器 validation_loader 的长度乘以重复次数。</p><p>生成预测结果<br>进行模型的验证过程，它使用模型对验证数据集进行预测，并收集相关的预测结果和真实值，以便后续的性能评估。代码重复了 repeat_num 次验证过程。这是为了获取更稳定的性能评估结果。<br>使用模型的输出（log_normal_mu 和 log_normal_sigma2）以及采样策略来生成样本的注视点向量。这通过调用 sampling.random_sample 函数完成。<br>对于验证集中的每个图像（由 N 表示），代码生成一个包含多个键值对的字典 predict_result。这些键值对包括图像名称、任务名称、重复ID、X和Y坐标的注视点位置、时间戳（乘以1000，可能是为了转换为毫秒）以及注视点的数量。这些预测结果存储在 predict_results 列表中，用于后续保存或分析。<br>更新进度条，表示完成一次预测。使用 evaluation 函数评估模型性能，该函数接收真实的注视点向量和预测的注视点向量作为输入，并返回评估指标和它们的标准差。<br>评估结果存储在 cur_metrics 和 cur_metrics_std 中。保存预测结果，记录评估结果。</p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 在视觉问答中预测人类的视觉扫描路径 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Predicting Goal-directed Human Attention Using Inverse Reinforcement Learning</title>
      <link href="/2023/10/24/GroupEight/"/>
      <url>/2023/10/24/GroupEight/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Predicting Goal-directed Human Attention Using Inverse Reinforcement Learning<br>使用逆强化学习预测目标导向的人类注意力</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>预测人类凝视行为对于行为视觉和计算机视觉应用</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>于洋 娄之茵</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="论文题目"><a href="#论文题目" class="headerlink" title="论文题目"></a>论文题目</h4><p><img src="/images/8/1-1.png"><br>文献基本信息：本文要预测人类凝视行为对于行为视觉和计算机视觉应用。在图像分类、目标检测和图像分割等任务中，逆向强化学习框架能提高模型性能。它模拟观察者的真实注视模式，使模型更关注图像的重要区域，从而提高准确性和效率。逆向强化学习框架也可应用于自然语言处理、语音识别等领域。在这些领域中，任务常涉及序列数据的处理和建模，而逆向强化学习框架提供了一种有效的序列建模方法。</p><h4 id="研究什么："><a href="#研究什么：" class="headerlink" title="研究什么："></a>研究什么：</h4><p>据我们所知，人类视觉注意力有两种形式：<br>1.自下而上的基于视觉输入处理<br>这种形式的注意力通常是由外部刺激引起的，例如颜色、形状、大小等。它基于图像的特征进行注意力的分配，不需要任何先验知识或目标导向。<br>2.自上而下的基于优先级处理<br>这种形式的注意力通常是由内部目标或任务驱动的。它根据个体的经验、目标和意图，对图像的不同区域进行优先级的分配，从而引导注意力的定向。<br>当您的食物到达餐厅时，您的第一个动作注意力可能会集中在叉子和刀子上，因为这些物品对于您的晚餐目标很重要。<br><img src="/images/8/1-2.png"></p><h4 id="要做什么：预测出人类注意力扫描路径。"><a href="#要做什么：预测出人类注意力扫描路径。" class="headerlink" title="要做什么：预测出人类注意力扫描路径。"></a>要做什么：预测出人类注意力扫描路径。</h4><p>视觉搜索中的注视预测。目的是预测人们在观看图像时的注视模式。这些模式可以是空间（注视密度图）或空间+时间（扫描路径）。大多数注视预测模型都是在自由观看任务的背景下进行的。</p><h4 id="IRL模型-一种模仿学习："><a href="#IRL模型-一种模仿学习：" class="headerlink" title="IRL模型  一种模仿学习："></a>IRL模型  一种模仿学习：</h4><p>逆向强化学习是一种通过观察行为并推断出产生这些行为的奖励函数的方法。在搜索预测中，我们可以利用逆向强化学习从观察者的行为中学习他们的搜索策略，并预测他们未来的搜索行为。那逆强化学习和强化学习有什么区别呢？强化学习主要应用于机器人控制、游戏等领域，其中智能体需要在动态环境中自主学习和决策。逆强化学习更适用于从人类行为中学习，例如模仿学习、行为理解等。强化学习是通过试错的方式学习最优策略，而逆向强化学习则是通过观察行为并推断出奖励函数，进而推断出最优策略。通俗来讲，与传统的强化学习相比，逆强化学习多了一个解释性的角度，即不仅要做出正确的动作，还要理解为什么要这么做。<br><img src="/images/8/1-3.png"><br><img src="/images/8/1-4.png"><br><img src="/images/8/1-5.png"></p><h4 id="Dynamic-Contextual-Belief-DCB-："><a href="#Dynamic-Contextual-Belief-DCB-：" class="headerlink" title="Dynamic-Contextual-Belief (DCB)："></a>Dynamic-Contextual-Belief (DCB)：</h4><p>动态上下文信念（DCB）：DCB包含观察者状态、任务上下文、图像特征等要素，有三个组成部分1.1) Fovea（中央凹：中央凹之外的视觉输入分辨率较低，模糊程度取决于周边观察输入与中央凹之间的距离。）2.情境信念3.动态（指每次注视后发生的状态表示的变化）。<br>这些在观察者的注视过程中不断积累和更新，指导其下一步的注视位置。基于逆向强化学习进行更新。比较实际与预测的注视位置，并考虑任务上下文和图像特征，持续更新DCB状态。<br><img src="/images/8/1-6.png"><br><img src="/images/8/1-7.png"></p><h4 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h4><p>COCO-Search18数据集，提供了大规模、多样化的数据集，用于研究目标导向的注意力机制。模拟人类视觉搜索过程，有助于理解人类注意力分配方式，并在计算机视觉系统中实现类似功能。标注的信息包括每个人在寻找目标物体时的注视位置、时间，以及完成任务的成功率。在实验设置中，我们使用了COCO-Search18数据集，该数据集包含了10个人在寻找18个目标物体时的标注信息。我们基于这些标注信息，利用逆向强化学习算法，训练了一个预测模型，该模型可以预测人类在寻找特定目标时的注视模式。<br>COCO-Search18数据集用在标准目标存在（TP）或目标不存在（TA）搜索任务期间进行的人类注视来注释COCO，其中在每次试验中，搜索图像要么描绘目标（TP）要么没有明显目标（TA）。<br><img src="/images/8/1-8.png"></p><h4 id="Reward-and-Policy-Learning"><a href="#Reward-and-Policy-Learning" class="headerlink" title="Reward and Policy Learning"></a>Reward and Policy Learning</h4><p>我们使用生成对抗模仿学习（GAIL）来学习视觉搜索行为的奖励函数和策略。GAIL是一个带有判别器和生成器的对抗框架。该策略是生成器，旨在生成类似于人类行为的状态-动作对。奖励函数将状态-动作对映射为数值，并且该函数被公式化为鉴别器输出的对数。<br>生成器的作用：通过采样图像和任务中的眼动，生成假状态动作对，用于辨别器进行学习和预测。这些假状态动作对基于逆向强化学习框架生成，模拟观察者的真实注视过程。欺骗判别器。<br>辨别器的作用：通过区分真假状态动作对，学习并预测观察者的真实注视模式。利用生成器提供的假状态动作对和观察者真实的眼动数据，比较真假数据差异，不断优化预测能力。辨认出生成器生成的动作对和真实的数据。<br>在训练过程中，生成器和辨别器相互交互和迭代，共同优化整个逆向强化学习框架的性能。</p><h4 id="贡献以及意义："><a href="#贡献以及意义：" class="headerlink" title="贡献以及意义："></a>贡献以及意义：</h4><p>提出了一种预测搜索定点扫描路径的新模型，该模型使用 IRL 来联合恢复人们在视觉搜索过程中使用的奖励函数和策略。IRL 模型使用了一种新颖且具有高度解释能力的状态表示法–动态上下文信念（DCB），DCB 会更新对物体的信念，以获得随每次新的定点而动态变化的物体上下文。<br>IRL模型  (i) 了解对象的场景上下文； (ii) 泛化以预测新受试者 的行为，(iii) 与其他模型相比，需要更少的数据来实现良好的性能。<br>最后，我们学习了如何量化搜索任务中注视点的奖励函数。这将使新一波的实验研究成为可能，最终将更好地理解目标导向的注意力。<br>对行为视觉文献产生了影响，因为对于真实图像来说，引导人类目标导向型注意力的视觉特征仍然鲜为人知。</p><h4 id="想要完成的："><a href="#想要完成的：" class="headerlink" title="想要完成的："></a>想要完成的：</h4><p>模型实现（<a href="https://github.com/cvlab-stonybrook/Scanpath_Prediction%EF%BC%89%EF%BC%9A">https://github.com/cvlab-stonybrook/Scanpath_Prediction）：</a><br>1.必要的库和模块：<br>·detectron2：一个用于目标检测和语义分割的库。<br>·torch：PyTorch 是一个流行的深度学习框架，用于构建神经网络模型和进行数值计算。<br>·tqdm：一个用于在命令行界面中显示进度条的库，提供了一种更直观的方式来监控长时间运行的任务。<br>·tensorboard：TensorBoard 是 TensorFlow 生态系统中的一部分，用于可视化模型训练过程中的各种指标和数据。<br>·torchvision：这是 PyTorch 的视觉处理库，提供了常见的图像预处理、数据加载和模型架构。<br>·numpy：Numerical Python 是一个用于科学计算的基础库，提供了多维数组和矩阵操作的功能。<br>·scipy：Scientific Python 是一个用于数值计算和科学分析的库，包含了各种数学、统计和优化函数。<br>·pillow：Pillow 是 Python 中常用的图像处理库，用于图像的读取、编辑和保存。<br>·docopt：用于解析命令行参数的库，使得编写命令行工具更加简单。<br>·scikit-image：一个用于图像处理和分析的库，提供了丰富的图像变换和算法。<br>2.使用以下命令训练模型：<br>python train.py <hparams> <dataset_root> [–cuda=<id>]<br>绘制扫视路径的命令：<br>python plot_scanpath.py –fixation_path <fixation_file_path> –image_dir <image_dir><br>3.如何使用已经训练好的模型为新图像生成扫描路径（不在 COCO-Search18 中）<br>[1]计算 DCB<br>运行文件中的extract_DCBs_demo.py<br>主程序部分：<br>·加载预训练的模型和配置。<br>·根据配置构建骨干网络并将其移动到指定设备（cuda）。<br>·设置模型权重并进行评估模式。<br>·使用给定的图像路径和预测器计算高对比度和低对比度的特征。<br>·打印特征的形状。<br>遇到的问题：detectron2的安装（已解决）<br>运行代码后的结果：？<br><img src="/images/8/1-9.png"><br>[2]运行作者在github中给出的代码<br><img src="/images/8/1-10.png"><br><font color="red" size="4"> 遇到的问题：checkpoint_dir是什么 </font> </image_dir></fixation_file_path></id></dataset_root></hparams></p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><p>首先安装软件以及对应库</p><pre class="line-numbers language-none"><code class="language-none">有关此模型需要的库：torchtqdmtensorboardtorchvisionnumpyscipypillowdocoptscikit-image<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>我们又下载了数据库detectron2（README中详细提及）<br>使用Detectron2，需要安装：对应版本pytorch、cuda、Visual Studio 2019<br>数据库应按照作者README中提到的数据库格式进行设置：<br><img src="/images/8/2-1.png"><br>使用作者发布的数据集processed，格式如下：<br><img src="/images/8/2-2.png"><br><img src="/images/8/2-3.png"><br>首先，运行train.py，格式如上。<br>因目的为实现已训练好的模型，故暂时跳过此步骤。</p><ol><li><img src="/images/8/2-4.png">这段代码主要实现了两个功能：坐标转换和扫描路径的绘制。<br>               Fixation_path示例:</li></ol><p><img src="/images/8/2-5.png"><br><img src="/images/8/2-6.png"><br>1.坐标转换函数&nbsp;convert_coordinate：<br>这个函数接受四个参数：X&nbsp;和&nbsp;Y&nbsp;是注视点的坐标，im_w&nbsp;和&nbsp;im_h&nbsp;是图像的宽度和高度。函数的目标是将这些坐标从显示坐标转换为像素坐标。</p><pre class="line-numbers language-none"><code class="language-none">* 首先，定义了一个显示屏幕的目标宽高 `display_w` 和 `display_h`。 * 然后，计算了图像和目标显示屏幕的宽高比，以判断在显示屏幕上图像应该如何缩放。 * 如果图像的宽高比大于目标屏幕的宽高比，那么图像将被缩放到屏幕的宽度，并相应地调整高度。反之，图像将被缩放到屏幕的高度，并相应地调整宽度。 * 计算了缩放后图像与原始屏幕之间的差异（`delta_w` 和 `delta_h`），然后计算了缩放比例 `scale`。 * 最后，使用计算出的缩放比例和差异，将 `X` 和 `Y` 坐标转换为新的像素坐标。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>2.&nbsp;扫描路径绘制函数&nbsp;plot_scanpath：<br>这个函数用于绘制注视点的扫描路径。它接受以下参数：图像&nbsp;img，注视点的&nbsp;x&nbsp;坐标&nbsp;xs，注视点的&nbsp;y&nbsp;坐标&nbsp;ys，注视点的时间戳&nbsp;ts，以及一个可选的边界框&nbsp;bbox&nbsp;和标题&nbsp;title。</p><pre class="line-numbers language-none"><code class="language-none">* 首先，使用 `plt.subplots()` 创建了一个新的图像和坐标轴。 * 使用 `ax.imshow(img)` 在坐标轴上显示了图像。 * 定义了一些用于绘制扫描路径的参数，如圆的最小和最大半径，以及时间戳的最小和最大值。 * 使用 `plt.arrow` 在坐标轴上绘制了注视点之间的箭头，表示扫描路径。 * 对于每个注视点，根据时间戳计算了圆的半径，并使用 `plt.Circle` 在坐标轴上绘制了一个圆。同时，使用 `plt.annotate` 在每个圆上添加了注释，显示注视点的顺序。 * 如果提供了边界框 `bbox`，则使用 `Rectangle` 在坐标轴上绘制一个矩形框。运行结果示例：<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/8/2-7.png"></p><ol start="2"><li><p><img src="/images/8/2-8.png"><br>这段代码用于计算Dynamic-Contextual-Belief (DCB).，主要使用了Detectron2库。Detectron2是Facebook AI Research开发的一个目标检测、分割和姿态估计库，它建立在PyTorch之上。<br>1.导入依赖库：<br>otorch&nbsp;和&nbsp;torch.nn.functional：PyTorch库，用于深度学习模型的构建和计算。<br>onumpy：用于数组操作。<br>oPIL&nbsp;和&nbsp;ImageFilter：用于图像处理。<br>odetectron2&nbsp;和相关模块：用于构建和加载预训练的目标检测和分割模型。<br>2.设备设置：<br>odevice = torch.device(“cuda”)：设置PyTorch使用CUDA进行GPU加速。<br>3.定义&nbsp;pred2feat&nbsp;函数：<br>o这个函数将分割结果转换为特征表示。它接受分割结果&nbsp;seg&nbsp;和信息&nbsp;info&nbsp;作为输入，并输出一个经过插值的特征图。特征图的每个通道对应一个类别，如果该位置属于该类别，则值等于分割得分或1（取决于是否为“thing”类别）。<br>4.定义&nbsp;get_DCBs&nbsp;函数：<br>o这个函数用于计算图像的深度上下文偏差（DCBs）。它首先对图像进行高斯模糊处理，然后使用Detectron2的预训练模型对原始图像和模糊后的图像进行分割。然后，它使用&nbsp;pred2feat&nbsp;函数将分割结果转换为特征，并返回原始图像和模糊图像的特征。<br>5.主程序：<br>o加载Detectron2的配置文件，使用 Detectron2 的预测器 predictor 对输入图像进行 panoptic 分割，得到高分辨率和低分辨率的 panoptic 分割结果。<br>o将 panoptic 分割结果转换为特征图，其中物体类别的位置填充得分，非物体类别的位置填充二元掩码。<br>o对特征图进行插值，使其尺寸统一为 [20, 32]。<br>o返回高分辨率和低分辨率 DCB 特征图。<br>保存结果：<br><img src="/images/8/2-9.png"></p></li><li><p><img src="/images/8/2-10.png"><br>这段代码是一个用于处理训练数据和验证数据的函数，名为&nbsp;process_data。这个函数的主要任务是为模型训练准备数据，包括预处理注视点数据、提取图像数据以及生成一些辅助信息。</p></li></ol><p>模型评估，使用作者给出的代码以及数据集实现代码功能：<br>用法:<br>    test.py <hparams> <checkpoint_dir> <dataset_root> [–cuda=<id>]<br>    test.py -h | –help<br><img src="/images/8/2-11.png"><br>其中，checkpoint<br><img src="/images/8/2-12.png"><br>为：<br><img src="/images/8/2-13.png"><br>Dataset_root使用作者给出的数据集processed。<br>更改的地方：源代码中，使用多线程数据加载，而我所使用的笔记本性能不足以支撑，报错多次后发现问题根源，更改为单线程后顺利运行。<br>运行结果：<br><img src="/images/8/2-14.png"><br><img src="/images/8/2-15.png"><br><img src="/images/8/2-16.png"></id></dataset_root></checkpoint_dir></hparams></p><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="背景与意义"><a href="#背景与意义" class="headerlink" title="背景与意义"></a>背景与意义</h4><p>1-1选题背景与意义<br>逆强化学习的提出：<br>信息时代<br>快速聚焦<br>人工智能<br>信息处理<br>先前技术<br><img src="/images/8/3-1.png"><br>传统的基于显著性图的模型在预测自由观看行为方面取得了显著成果，但在预测目标导向的注视行为时存在局限性。因此，我们提出利用逆强化学习的方法，从人类视觉搜索的行为数据中学习出内部的奖励函数和策略，进而实现对目标导向注意力的预测。</p><h4 id="思路与方法"><a href="#思路与方法" class="headerlink" title="思路与方法"></a>思路与方法</h4><p>2-1 理论基础<br><img src="/images/8/3-2.png"><br>逆强化学习（Inverse Reinforcement Learning，IRL）是一种机器学习技术，它的目标是从观察到的行为（通常是专家或人类的行为）中推断出潜在的奖励函数。</p><p>这与传统的强化学习（Reinforcement Learning，RL）正好相反，传统RL中奖励函数是预先定义的，而学习目标是找到最大化这个奖励函数的行为策略。</p><p>逆强化学习（Inverse Reinforcement Learning，IRL）和强化学习（Reinforcement Learning，RL）都是机器学习领域中的重要研究方向，它们都旨在解决智能体在与环境交互过程中的学习和决策问题。</p><p>IRL 的主要优势在于它可以从专家演示中学习奖励函数，而不需要手动设计奖励函数。这对于一些复杂的任务或领域，如人类行为预测、机器人控制等，可能更加有效。通过观察专家的行为，IRL 可以学习到专家的策略和决策模式，从而更好地理解和预测人类行为。</p><p>相比之下，RL 则需要手动设计奖励函数，这可能需要对问题有深入的理解和经验。此外，RL 通常需要大量的试错和探索，以找到最优的策略。</p><p>但IRL需要大量的专家演示数据，并且对于一些复杂的任务，可能难以获得高质量的演示数据。因此，数据集的选择至关重要。</p><p>2-2 研究过程<br>1.理解逆强化学习（IRL）：<br>    1.逆强化学习是一种从观察到的行为中学习奖励函数或策略的方法。与传统的强化学习相反，IRL从观察到的行为（如人类搜索目标的行为）中学习，而不是预先定义奖励函数。<br>    2.在研究中，要理解如何使用IRL来从人类搜索数据中学习注意力分配的奖励函数或策略。<br>2.数据来源：<br>    1.为了训练IRL模型，需要一个包含人类搜索行为的数据集。我们通过网站下载使用了作者提到的COCO-Search18数据集，他是一个包含6202张不同类别图像的数据集。<br>    2.需要确保能够访问这些数据集，并理解其格式和内容，理解作者代码中对其进行的处理。<br>3.模型构建：<br>    1.使用Python和对应机器学习库（本文使用pytorch），安装必须的依赖库。搭建相应环境。<br>    2.安装pytorch、visual c++、cuda。（需与GPU版本对应）。<br>    3.需要考虑如何设计模型的架构，以便能够处理输入数据（如图像和人类搜索路径）并输出预测的奖励函数或策略。<br>4.模型训练：<br>    1.使用数据集训练IRL模型。这通常涉及多个迭代，其中模型尝试从观察到的行为中学习奖励函数或策略。在逆强化学习中，奖励函数是关键。需要根据人类搜索行为的特点和数据集的特性，设计适当的奖励函数形式或结构。<br>    2.在训练过程中，可能需要调整模型的超参数（如学习率、批次大小等）以优化性能。<br>5.模型评估：<br>    1. 一旦模型训练完成，需要评估其在预测目标导向的人类注意力方面的性能。<br>    2.这可以通过与人类搜索行为的相似性、搜索效率等指标来衡量。<br>    3.与基线模型进行比较，以展示IRL模型的优势。<br>6.代码复现：<br>    1.需要仔细阅读他们的论文和代码，并理解其方法。<br>    2.安装相同的库和依赖项，并确保数据预处理和模型训练步骤与原始研究一致。下载安装detectron2，便于绘制结果的可视化。<br>    3.并且，需要根据自己所使用的设备进行调整。例如，源代码中，使用多线程数据加载，而我所使用的笔记本性能不足以支撑，报错多次后发现问题根源，更改为单线程后顺利运行。<br>7.结果分析和讨论：<br>    1.分析模型在测试集上的性能，并讨论其优点和局限性。<br>    2.考虑如何改进模型以进一步提高性能，并讨论未来的研究方向。<br>8.撰写论文或报告</p><h4 id="关键技术难点"><a href="#关键技术难点" class="headerlink" title="关键技术难点"></a>关键技术难点</h4><p>3-1 关键技术<br>1.逆强化学习算法的设计<br>一种机器学习技术，它的目标是从观察到的行为（通常是专家或人类的行为）中推断出潜在的奖励函数。<br>2.动态上下文信念图<br>动态上下文信念映射可能是一种用于表示和更新关于环境中目标位置信念的方法。在视觉搜索任务中，它可能表示搜索者对目标物体可能位置的信念。<br>3.大规模数据集<br>选择了COCO-Search18数据集，这是一个由10名参与者在6202张图像中搜索18个不同目标物体类别的大规模行为数据集，包含了约300,000个目标导向的注视点。这个数据集的规模和多样性使得我们能够训练和评估模型在多种任务环境中的表现，同时也能保证实验结果的可靠性。其中，每个实验任务都要求参与者在图像中找到特定的目标物体，记录下的数据包括参与者在搜索过程中视线的移动轨迹，这些轨迹被转化为注意力分配的序列，作为模型训练的输入。<br>4.行为扫描路径预测<br>行为扫描路径预测是指根据已知的信息（如搜索者的内部状态、奖励函数、视觉输入等）预测搜索者未来可能的眼睛注视路径。</p><p>3-2 实践难点<br>我们提出了第一个逆强化学习（IRL）模型，用于学习人类在视觉搜索过程中使用的内部奖励函数和策略。观察者的内部信念状态被建模为对象位置的动态上下文信念图。这些图是通过 IRL 学习的，然后用于预测多个目标类别的行为扫描路径。为了训练和评估我们的 IRL 模型，我们创建了 COCO-Search18，这是目前存在的最大的高质量搜索注视点数据集。COCO-Search18 有 10 名参与者在 6202 张图像中搜索 18 个目标对象类别，产生了约 30 万个目标导向的注视点。在 COCO-Search18 上进行训练和评估时 ，无论是在与人类搜索行为的相似性还是搜索效率方面， IRL 模型在预测搜索注视点扫描路径方面都优于基线模型。<br><img src="/images/8/3-3.png"><br><img src="/images/8/3-4.png"><br><img src="/images/8/3-5.png"><br>数据集由两部分组成：图像刺激和注视点。为了提高计算效率，我们使用来自 Detectron2 的预训练全景 FPN（使用 ResNet50 骨干）预先计算低分辨率和高分辨率的信念图。对于每个图像，我们提取 134 个低分辨率和高分辨率的信念图，并将它们调整为 20x32。因此，对于每个图像，我们有两个 134x20x32 张量。注视点以单个扫描路径的形式出现，主要由图像坐标中的（x，y）位置列表组成。在原始注视点中可能存在超出图像边界的注视点，需要将其从扫描路径中删除。<br><img src="/images/8/3-6.png"></p><pre class="line-numbers language-none"><code class="language-none">    # 加载图像并进行预处理    high = Image.open(img_path).convert('RGB').resize((512, 320))  # 高分辨率图像    low = high.filter(ImageFilter.GaussianBlur(radius=radius))  # 低分辨率图像（高斯模糊）&nbsp;    # 使用 Detectron2 预测器获取高分辨率和低分辨率的 panoptic 分割结果    high_panoptic_seg, high_segments_info = predictor(np.array(high))["panoptic_seg"]    low_panoptic_seg, low_segments_info = predictor(np.array(low))["panoptic_seg"]&nbsp;    # 将 panoptic 分割结果转换为特征图    high_feat = pred2feat(high_panoptic_seg, high_segments_info)  # 高分辨率 DCB 特征图    low_feat = pred2feat(low_panoptic_seg, low_segments_info)  # 低分辨率 DCB 特征图&nbsp;    return high_feat, low_feat&nbsp;if __name__ == '__main__':    # 加载预训练的 panoptic_fpn 模型    cfg = get_cfg()    cfg.merge_from_file('/home/zhibyang/github/detectron2/configs/COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml')    model = build_backbone(cfg).to(device)    model.eval()&nbsp;    # 使用预训练的模型参数    cfg.MODEL.WEIGHTS = 'detectron2://COCO-PanopticSegmentation/panoptic_fpn_R_50_3x/139514569/model_final_c10459.pkl'    model_coco = build_backbone(cfg).to(device)    model_coco.eval()    predictor = DefaultPredictor(cfg)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/8/3-7.png" alt="人类扫描路径.json文件格式"><br><img src="/images/8/3-8.png"></p><h4 id="成果与总结"><a href="#成果与总结" class="headerlink" title="成果与总结"></a>成果与总结</h4><p>4-1 成果展示<br>研究成果形式<br>实验结果显示，该模型无论是在任务的复杂性和多变性上，还是在预测注意力转移和聚焦的细节上都展现出了明显的优势。这表明，这一逆强化学习框架能够更准确地捕捉到人类在实际任务中注意力的动态变化，从而模拟出更接近真实的行为模式。该模型预测出的注意力模式与心理学理论的预测结果高度契合，例如，模型揭示的目标依赖的对象优先级模式，与自上而下的注意力模型的理论预测相吻合。这证明了此方法不仅能够对注意力行为进行表层的模仿，还能深入理解其内在的心理机制，从而提供了一种理解和预测注意力行为的强大工具。<br><img src="/images/8/3-9.png"></p><p>4-2 发展展望<br>自动驾驶与智能交通：研究中的注意力预测模型可以用于增强驾驶辅助系统，通过预测驾驶员的注意力分布，系统可以提前预测可能的分心行为，从而提供预警，降低交通事故风险。此外，结合环境信息，如天气、交通状况，模型还能帮助优化车内环境，提升驾驶员的专注度。<br>智能交互与用户体验设计：在虚拟现实、增强现实及游戏设计中，我们的模型可以实时分析用户在场景中的注意力焦点，帮助优化内容的呈现，如动态调整画面元素，提高沉浸感和交互体验。在教育应用中，通过预测学生在学习过程中的注意力分布，可以实现个性化教学内容的自适应调整，提高学习效率。</p><p>心理健康与认知评估：注意力预测模型可以作为评估工具，帮助心理学家和临床医生识别注意力缺陷，尤其是在多动障碍（ADHD）等疾病的诊断上。通过分析注意力模式，可以更精确地了解个体的注意力集中能力，为制定个性化的治疗方案提供依据。<br>内容推荐系统：在信息过载的今天，注意力预测模型可以帮助推荐系统预测用户的兴趣点，从而提供更个性化的信息流。通过理解用户在不同内容上的关注度，系统可以动态调整推荐策略，提高用户对推荐内容的接受度。</p><p>教育领域:根据学生的目标和注意力倾向，进行个性化学习设计，为其定制最适合的学习内容和顺序。同时，预测学生可能对哪些教学资源更感兴趣、更能集中注意力，从而进行精准推荐。对于教师来说，可以了解学生注意力的分布，针对性地调整教学方式和互动环节，以更好地吸引学生。教室在设计教材或课件时，可以依据对学生注意力的预测，合理安排重点内容的呈现方式和位置。这一技术还可以帮助教师实时监测学生在课堂上的注意力状态，以便及时采取措施改善学习氛围。<br>通过将注意力预测技术应用到这些领域，不仅能提供更人性化的交互体验，还能为解决实际问题提供科学依据，推动相关领域的技术革新。为了实现这些应用，仍需需要进一步优化模型的实时性、适应性，以及如何在保护用户隐私的前提下，更有效地利用用户数据实现个性化。同时，与业界的紧密合作，尤其是在数据共享和伦理规范上，也将是确保技术成功应用的关键。</p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 使用逆强化学习预测目标导向的人类注意力 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OOD</title>
      <link href="/2023/10/23/GroupNine/"/>
      <url>/2023/10/23/GroupNine/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>OOD<br>分布外</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>改进机器学习系统在处理分布外数据（OOD）时的性能，提高模型的鲁棒性，使其能够更好地适应和泛化到未见过的数据。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>王月仙 邹芳冉</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="一、论文研究与理解"><a href="#一、论文研究与理解" class="headerlink" title="一、论文研究与理解"></a>一、论文研究与理解</h4><p>1、研究背景<br>人们对于机器学习系统在分布外数据（OOD）上的推广能力越来越感兴趣。然而在优化过程中 OOD 目标和 ERM 之间往往存在着冲突，需要在 OOD 目标和经验风险最小化（ERM）之间做出妥协，这可能会削弱模型的鲁棒性。<br>OOD：模型能够在分布外的数据上表现良好而设定的优化目标。<br>ERM:指在面临不确定性和风险的情况下，通过积累经验和应用经验来减少可能得损失和负面影响。<br>OOD 与 ERM 之间的冲突：<br>（1）数据范围不同，ERM 的优化模型实在已有的数据上的性能，OOD 实在没见过的数据上的性能<br>（2）过拟合风险，为了最小化 ERM，模型会过度你和训练数据集，导致对未见过的数据上的表现不佳，无法泛化。<br>（3）数据偏差，训练数据和测试数据很难保证一致性<br>2、研究目的<br>改进机器学习系统在处理分布外数据（OOD）时的性能，提高模型的鲁棒性，使其能够更好地适应和泛化到未见过的数据。解决在优化过程中出现的妥协问题，提高模型在 OOD 数据上的性能，并取得更好的鲁棒性和泛化能力。<br>3、解决的问题<br>在优化过程中往往需要在 OOD 目标和经验风险最小化（ERM）之间做出妥协，这可能会削弱模型的鲁棒性，导致次优性能的问题。<br>4、使用的方法<br>帕累托不变风险最小化（PAIR）的新型优化方案，提出了一个新的优化器PAIR-o 和一个新的模型选择标准 PAIR-s 来缓解这一困境。PAIR 通过与其他 OOD目标协同优化来提高鲁棒性，同时适当权衡 ERM 和 OOD 目标，接近帕累托最优解。<br>5、结论<br>PAIR 在挑战性基准测试中取得了最高的 OOD 性能。从 MOO 的角度对 OOD泛化中的优化困境进行了新的理解，并将 OOD 优化的失败归因于宽松 OOD 目标的鲁棒性受损和不可靠的优化方案。强调了权衡 ERM 和 OOD 目标的重要性，并提出了一个新的优化器 PAIR-o 和一个新的模型选择标准 PAIR-s 来缓解这一困境。提供广泛的理论和实证证据来证明正确处理 ERM 和 OOD 权衡的必要和重要性。<br>6、总结与展望<br>随着深度学习模型在现实世界中的广泛应用，对模型在未见过的数据上的泛化能力和鲁棒性的需求也越来越高。OOD 技术可以帮助识别模型在测试时遇到的未知数据，从而提高模型的安全性、可靠性和实用性。未来，我们可能会看到OOD 技术在计算机视觉、自然语言处理、智能机器人自动驾驶、医疗诊断、金融风险管理等领域得到更广泛的应用，以确保深度学习模型在复杂环境中的稳健性和可靠性。通过进一步探索和改进 PAIR 的方法，可以期望在处理分布外数据的任务中取得更好的结果。随着对 OOD 问题的深入研究和不断的改进，机器学习系统在未知领域和新数据上展现出更高的鲁棒性和泛化能力，为实际应用带来更大的效益和可靠性</p><h4 id="二、项目准备"><a href="#二、项目准备" class="headerlink" title="二、项目准备"></a>二、项目准备</h4><p>在这个月内我们进行了环境配置的相关工作：<br>⚫ 安装 Anaconda<br><img src="/images/9/1-1.png"><br>⚫ 配置 Anaconda 的环境变量和 Anaconda 虚拟环境<br><img src="/images/9/1-2.png"><br>⚫ 设置 Jupyter Notebook<br><img src="/images/9/1-3.png"><br>⚫ 安装 GPU 版本的 PyTorch 库<br><img src="/images/9/1-4.png"><br><img src="/images/9/1-5.png"></p><h4 id="三、代码初步理解"><a href="#三、代码初步理解" class="headerlink" title="三、代码初步理解"></a>三、代码初步理解</h4><p>我们对部分代码进行了逐行逐块的分析与理解，例如：<br><img src="/images/9/1-6.png"><br><img src="/images/9/1-7.png"></p><h4 id="四、总结与展望"><a href="#四、总结与展望" class="headerlink" title="四、总结与展望"></a>四、总结与展望</h4><p>以上是我们这一阶段所做的工作，下一阶段我们将会继续分析代码，弄清楚各个 py 文件之间的联系，梳理网络框架和神经网络模型，并开始尝试运行代码</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h4 id="一、数据集来源"><a href="#一、数据集来源" class="headerlink" title="一、数据集来源"></a>一、数据集来源</h4><p>1、生成方式：数据是通过 Python 中的 numpy 库随机生成的。具体地，使用了 numpy 的 random.multivariate_normal 函数，这个函数可以生成具有特定均值和协方差的高斯（正态）分布的数据。<br>2、训练数据：生成的训练数据具有均值 [2, 3] 和协方差矩阵 [[1, 0.5], [0.5, 1]]。这种设置模拟了模型在实际应用中可能遇到的常规数据分布。<br>3、OOD 数据：生成的 OOD 数据具有不同的均值 [0, 0] 和协方差矩阵 [[1, 0], [0, 1]]。这种不同的设置用于模拟模型在实际应用中可能遇到的异常或未知的数据分布，以测试模型对于分布外数据的响应。</p><h4 id="二、网络输入"><a href="#二、网络输入" class="headerlink" title="二、网络输入"></a>二、网络输入</h4><p>特征维度：网络的输入是二维的数据点。每个数据点由两个特征组成，这对应于生成数据时每个样本的两个维度。</p><h4 id="三、网络输出"><a href="#三、网络输出" class="headerlink" title="三、网络输出"></a>三、网络输出</h4><p>输出维度：网络的输出是一个单一的连续值。这个输出可以用来进行回归分析，或者在更复杂的应用中用作其他类型的预测任务的基础。</p><h4 id="四、网络结构"><a href="#四、网络结构" class="headerlink" title="四、网络结构"></a>四、网络结构</h4><p>1、层级设置：<br>输入层：接受 2 个输入特征。<br>隐藏层：包含两个隐藏层，每层有 50 个神经元，并使用 ReLU 激活函数，以增加模型的非线性能力和处理复杂数据分布的能力。<br>输出层：最后一个全连接层将 50 个特征转换为一个单一的输出值。<br>模型架构代码：<br><img src="/images/9/2-1.png"><br>这个结构通过使用标准的层次配置，提供了足够的灵活性来捕捉从输入数据到输出的复杂映射。<br>2、训练过程<br>使用均方误差（MSE）作为损失函数，这是回归任务中常见的选择。优化器使用的是 Adam，这是一个广泛用于训练神经网络的优化算法，因其自适应学习率的特性而受到推崇。<br>模型在训练过程中逐渐调整其权重，以最小化输出与零的差异（这里假设了一个简化的目标，即尝试让模型输出接近于零）。<br>3、OOD 检测<br>方差比较：通过比较训练数据和 OOD 数据的预测输出的方差，来评估模型对 OOD 数据的敏感性。理论上，如果一个模型对训练数据过度拟合，那么它在OOD 数据上的预测将会有更大的方差，因为 OOD 数据引入了训练时未见的新特征或模式。<br>这个模型的设计和实现为 OOD 检测提供了一个基本的框架，可以根据特定的应用需求进行修改和扩展。通过训练和评估过程，可以进一步理解模型对常规和非常规数据的反应，从而优化模型的泛化能力和鲁棒性。</p><h4 id="五、训练结果"><a href="#五、训练结果" class="headerlink" title="五、训练结果"></a>五、训练结果</h4><p>模型已经完成了 100 个训练周期的训练，损失已经逐步减少到非常低的水平。<br>初始损失：约 0.3226<br>最终损失：约 0.00000519<br>OOD 检测结果<br>训练数据预测方差：约 0.0000041079<br>方差的差异显示出模型对于训练数据的预测比对 OOD 数据更为一致（方差较低），这意味着模型在训练数据上过拟合，而在 OOD 数据上显示出较大的变异性。<br>这种表现在实际应用中通常是期望的，因为它表明模型能够区分训练分布与OOD 分布。如果 OOD 数据的方差显著高于训练数据，这可能表明 OOD 数据在模型中表现出了不同的特征或行为模式。<br><img src="/images/9/2-2.png"><br>训练损失曲线显示了模型如何随着训练逐渐减少误差，这是模型学习和优化过程的体现。并且展示了模型在 100 个训练周期中损失的下降趋势。可以看到损失从一个较高的初始值迅速下降，随后平稳地接近于零，这表明模型在训练过程中逐渐拟合了数据。<br><img src="/images/9/2-3.png"><br>训练数据的预测分布（蓝色）：预测值集中在接近零的范围内，分布较窄，表明模型在训练数据上表现一致。<br>OOD 数据的预测分布（红色）：预测值分布更广，且中心偏离零，表明 OOD数据在模型上的表现与训练数据有明显差异。<br>预测分布图提供了对模型如何区分训练数据和 OOD 数据的直观了解。训练数据的预测值较为集中，而 OOD 数据的预测值分散，这显示了模型在面对不同分布时的反应差异。<br>这两个图表提供了模型性能的直观理解，尤其是在处理训练外数据时的表现，这对于评估模型的泛化能力和 OOD 检测能力非常有用。</p><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="实验与结果"><a href="#实验与结果" class="headerlink" title="实验与结果"></a>实验与结果</h4><p><img src="/images/9/3-1.png" alt="实验一"><br>此训练数据集为小狗实拍图片。<br>测试数据为三张小狗实拍图片和一张小狗素描，小狗素描为分布外数据，得出模型在未知数据中的相似程度。</p><p><img src="/images/9/3-2.png" alt="实验二"><br>此训练数据集为小狗实拍图片。<br>测试数据为三张小狗实拍图片和一张小猫，小猫为分布外数据，得出模型在未知数据中的相似程度。</p><p><img src="/images/9/3-3.png" alt="实验三"><br>此训练数据集为小狗、小猫、小鱼实拍图片。<br>测试数据为三张小狗、小猫、小鱼图片，小鸟为分布外数据，得出模型在未知数据中的表现，计算分布外数据与分布内数据的相似程度。</p><p><img src="/images/9/3-4.png" alt="实验四"><br>此训练数据集为小狗、小猫、小鱼实拍图片<br>测试数据为三张小狗、小猫、小鱼图片，小鸟为分布外数据，并给小狗、小猫、小鱼的数据集打上标签，得出小鸟数据在未知数据中的表现，得出分布外数据小鸟与这三类标签的相似程度。</p><p><img src="/images/9/3-5.png"><br>提供所有观察结果，进行异常值检查。</p><h4 id="数据集及代码"><a href="#数据集及代码" class="headerlink" title="数据集及代码"></a>数据集及代码</h4><p>1.数据<br>MLP（多层感知机）：大约50%的准确率。<br>传统的卷积神经网络（CNN）：60-80%的准确率。<br>现代深度学习架构（如ResNet、DenseNet）：超过95%的准确率。<br> 优势：<br>   数据量适中，适合用于快速原型设计和算法验证。<br>   类别均衡，每个类别都有相同数量的样本。<br>   广泛使用，许多文献和开源项目提供了丰富的基准和参考。</p><p>CIFAR-10数据集：是一个广泛使用的图像分类数据集，包含10个类别，每个类别有6000张32x32像素的彩色图像。类别包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。<br>ID数据：CIFAR-10的训练数据。训练模型时使用的所有数据点都被认为是分布内的样本。<br>例如，飞机、汽车、鸟、猫等。<br>OOD数据集：OOD数据集包含不在训练集中出现的类别或来自不同分布的图像。<br>训练数据：我们使用CIFAR-10的训练数据来训练模型，并计算每个类别的特征均值（类中心）。<br>测试数据：我们可以使用CIFAR-10的测试数据进行检测。</p><p>结果解释：<br>运行上述代码后，将输出ID样本和OOD样本的距离值。<br>CIFAR-10 Sample 0 Mahalanobis Distance: 5.23<br>CIFAR-10 Sample 1 Mahalanobis Distance: 4.78<br>…<br>SVHN Sample 0 Mahalanobis Distance: 15.47<br>SVHN Sample 1 Mahalanobis Distance: 16.12<br>…<br>ID样本的Mahalanobis距离值：[5.23, 4.78, …]<br>OOD样本的Mahalanobis距离值：[15.47, 16.12, …]<br>…<br>从这些距离值可以看出，ID样本的距离通常较小，而OOD样本的距离较大。通过设定一个阈值（如上例中的100.0），可以区分ID和OOD样本。如果距离超过阈值，则认为样本是OOD样本。。</p><p>2.研究方法<br>① 数据集选择：选择了CIFAR-10数据集作为训练和测试数据集，其中包含10个类别的彩色图像，用于模型的训练和评估。<br>②模型训练：使用选定的数据集对深度学习模型进行训练，以学习图像的特征和分类任务。<br>③OOD检测指标：采用Mahalanobis距离作为OOD检测的指标，通过计算样本在特征空间中与训练数据分布的距离来评估其是否为未知数据。<br>④阈值设定：根据Mahalanobis距离的结果，设定一个阈值来区分ID样本和OOD样本，通常超过阈值的样本被判定为OOD样本。<br>⑤实验验证：在训练数据集上训练模型，在测试数据集上进行实验验证，通过比较实际距离值和设定阈值来判断样本的分类情况，验证方法的有效性和准确性。<br>⑥结果分析：对实验结果进行分析和讨论，包括模型在ID样本和OOD样本上的表现，评估模型的鲁棒性和可靠性，并探讨方法的局限性和未来改进方向。</p><p><img src="/images/9/3-6.png"></p><p>3.研究目的与思路<br><img src="/images/9/3-7.png"></p><h4 id="研究目的与背景"><a href="#研究目的与背景" class="headerlink" title="研究目的与背景"></a>研究目的与背景</h4><p><img src="/images/9/3-8.png"><br><img src="/images/9/3-9.png"></p><h4 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h4><p><img src="/images/9/3-10.png"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分布外 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MotorControlLearning/SaccadeVR-mobile</title>
      <link href="/2023/10/22/GroupTen/"/>
      <url>/2023/10/22/GroupTen/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>MotorControlLearning/SaccadeVR-mobile<br>基于VR设备的眼动数据采集</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>利用VR技术所创造的沉浸式环境的优势，将基于视频的眼动跟踪技术与基于HMD的VR技术相结合，提高对眼球运动评估的准确率。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>叶蕾 肖芸 佟田润</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="一、研究主题与目的"><a href="#一、研究主题与目的" class="headerlink" title="一、研究主题与目的"></a>一、研究主题与目的</h4><p>1.1研究目的<br>（由论文MotorControlLearning/SaccadeVR-mobile）我们小组认为，可以利用VR技术所创造的沉浸式环境的优势，将基于视频的眼动跟踪技术与基于HMD的VR技术相结合，提高对眼球运动评估的准确率。因此，研究旨在研究一种组合设备HTC VIVE Pro Eye，以便将其用于对眼球运动轨迹的评估。<br>1.2研究内容概况<br>(1)利于unity平台和VIVE PRO Eye采集VR数据<br>(2)将采集后的数据在pytorch进行预处理和图像分类<br><img src="/images/10/1-1.png" alt="图 1 整个VR眼动采集+处理流程"></p><h4 id="二、研究设备与环境"><a href="#二、研究设备与环境" class="headerlink" title="二、研究设备与环境"></a>二、研究设备与环境</h4><p>2.1数据采集研究设备<br>HTC 公司推出了一款新的虚拟现实（VR）头戴式设备 VIVE Pro Eye，将基于红外线的眼球跟踪技术与VR技术结合在一起。<br><img src="/images/10/1-2.png" alt="图 2 VIVE Pro Eye设备"><br>测量系统使用了以下软件：<br>Unity:2019.2.5f1<br>Steam VR：1.11.11<br>SRanipal:由HTC提供的眼球跟踪软件开发工具包(SDK)：1.1.0.1<br>SR Runtim 1.1.2.0&lt;更新于2023-08-18&gt;.<br>我们确认该系统还可与以下更新软件版本配合使用。<br>眼部和面部跟踪SDK：1.3.6.8<br>SR运行时：1.3.6.11<br>眼部摄像头：2.41.0-942e3e<br>(在设置VIVE Pro Eye时下载了steamVR）<br>2.2数据处理所需环境和库<br>环境：Pycharm、IDLE<br>库：NumPy、Pandas、Pytorch等<br><img src="/images/10/1-3.png" alt="图 3 库"><br><img src="/images/10/1-4.png" alt="图 4 库"></p><h4 id="三、目前进度汇报"><a href="#三、目前进度汇报" class="headerlink" title="三、目前进度汇报"></a>三、目前进度汇报</h4><p><img src="/images/10/1-5.png" alt="图 5 采集概况"><br>目前已采集100张VR眼动图片数据，这里以2张及采集数据为例展示：<br><img src="/images/10/1-6.png" alt="图 6 眼动图片1"><br>(574.14, 693.87)(643.16, 538.28)(373.44, 525.74)<br>(574.82, 693.43)(663.79, 534.41)(155.50, 580.90)<br>(560.55, 692.38)(659.09, 533.25)(109.57, 594.00)<br>(533.87, 690.68)(656.45, 531.01)(117.89, 589.72)<br>(471.25, 685.39)(656.17, 530.85)(123.96, 585.73)<br>(465.10, 684.52)(654.45, 529.77)(137.37, 581.80)<br>(465.79, 684.13)(648.13, 530.62)(150.61, 579.15)<br>(484.69, 686.30)(646.61, 530.04)(185.15, 570.21)<br>(505.07, 688.17)(643.83, 531.00)(200.44, 565.74)<br>(536.22, 690.91)(635.90, 536.22)(215.03, 561.07)<br>(573.13, 694.22)(625.10, 533.07)(232.84, 555.91)<br>(584.35, 694.97)(621.14, 532.51)(238.01, 555.24)<br>(584.87, 694.15)(620.72, 531.77)(238.07, 553.35)<br>(588.50, 694.04)(625.90, 528.20)(235.80, 554.82)<br>(588.25, 693.35)(634.59, 526.18)(235.75, 556.49)<br>(590.72, 692.81)(642.16, 526.30)(236.23, 555.59)<br>(595.03, 692.46)(640.00, 525.27)(237.17, 554.84)<br>(595.63, 692.59)(635.45, 523.78)(241.13, 553.10)<br>(595.42, 694.08)(632.65, 524.39)(244.93, 553.77)<br>(574.38, 626.13)(659.83, 521.04)(255.76, 547.77)<br>(532.26, 594.53)(681.44, 518.73)(257.68, 547.61)<br>(499.05, 572.67)(700.53, 519.33)(254.65, 549.66)<br>(484.41, 542.33)(718.25, 516.74)(243.90, 550.03)<br>(489.09, 534.05)(716.82, 512.79)(236.01, 550.68)<br>(475.18, 529.08)(677.42, 510.52)(231.30, 552.87)<br>(479.50, 536.23)(647.83, 514.61)(247.90, 547.43)<br>(473.94, 534.67)(629.65, 512.13)(265.25, 543.28)<br>(484.42, 535.42)(726.27, 505.93)(381.41, 524.84)<br>(365.79, 518.70)(733.61, 507.00)(380.56, 527.09)<br>(405.94, 524.83)(743.78, 504.62)(381.19, 526.77)<br>(417.45, 525.81)(753.91, 495.49)(380.86, 525.89)<br>(437.02, 527.88)(568.75, 526.54)(387.62, 504.54)<br>(451.99, 530.20)(386.33, 549.38)(397.21, 531.24)<br>(457.98, 531.54)(263.69, 488.62)(448.68, 524.48)<br>(468.28, 530.22)(168.48, 450.81)(352.19, 544.72)<br>(468.95, 530.79)(175.07, 432.31)(449.36, 563.72)<br>(469.87, 530.71)(199.60, 420.15)(529.83, 528.17)<br>(451.24, 532.79)(189.28, 433.15)(595.38, 516.72)<br>(430.93, 528.34)(203.35, 423.18)(600.82, 533.81)<br>(411.09, 525.96)(382.88, 467.06)(621.57, 526.17)<br>(390.62, 525.22)(382.06, 467.03)(622.37, 537.17)<br>(387.42, 525.36)(398.44, 471.47)(608.71, 531.07)<br>(383.57, 527.37)(413.88, 474.50)(714.04, 515.03)<br>(375.99, 526.70)(431.60, 476.90)(813.15, 520.90)<br>(361.57, 523.58)(435.05, 476.84)(807.26, 529.13)<br>(342.42, 523.41)(442.70, 476.79)(799.77, 517.24)<br>(314.65, 521.66)(443.33, 476.82)(737.89, 520.02)<br>(312.73, 522.73)(445.36, 477.70)(641.35, 521.01)<br>(317.31, 525.98)(443.12, 474.60)(625.45, 514.52)<br>(353.50, 528.61)(444.01, 470.06)(617.70, 512.62)<br>(514.69, 541.21)(448.35, 471.48)(617.75, 507.36)<br>(512.64, 527.17)(447.73, 468.89)(795.18, 523.74)<br>(678.55, 548.65)(449.81, 470.25)(799.74, 523.27)<br>(749.56, 520.77)(448.55, 470.61)(802.29, 525.45)<br>(560.36, 539.59)(421.84, 468.18)(806.23, 527.59)<br>(530.53, 545.55)(410.80, 448.69)(807.37, 529.70)<br>(545.79, 547.48)(386.98, 475.27)(808.17, 531.66)<br>(547.33, 542.53)(379.76, 510.79)(789.88, 515.70)<br>(549.32, 542.27)(373.02, 523.66)<br>(713.02, 528.65)(369.40, 526.19)<br><img src="/images/10/1-7.png" alt="图 7 眼动图片2"><br>(357.97, 645.63)(524.28, 530.84)(139.19, 602.80)<br>(357.93, 645.64)(322.83, 521.95)(168.36, 602.75)<br>(357.34, 644.24)(300.19, 519.86)(198.60, 623.35)<br>(358.09, 646.92)(298.60, 517.96)(386.32, 562.33)<br>(421.21, 638.01)(297.46, 515.18)(387.99, 553.03)<br>(420.54, 638.03)(298.67, 514.08)(374.82, 556.01)<br>(401.56, 607.01)(314.59, 516.02)(314.93, 568.27)<br>(596.51, 617.50)(395.91, 521.34)(269.29, 577.40)<br>(600.65, 612.69)(407.30, 521.11)(269.82, 578.86)<br>(605.60, 645.73)(406.80, 520.40)(236.98, 582.68)<br>(589.66, 636.86)(406.49, 519.51)(222.37, 587.78)<br>(589.04, 600.68)(407.95, 516.76)(234.64, 583.66)<br>(577.26, 580.76)(416.93, 515.79)(263.55, 576.07)<br>(581.96, 634.40)(438.67, 515.31)(383.56, 550.99)<br>(589.60, 628.39)(446.94, 517.91)(400.63, 552.30)<br>(593.34, 639.35)(451.41, 519.85)(399.53, 547.45)<br>(547.00, 620.76)(450.38, 521.65)(400.28, 548.92)<br>(534.48, 613.40)(435.97, 521.41)(405.20, 548.06)<br>(517.14, 584.80)(424.52, 520.68)(404.73, 547.94)<br>(249.60, 653.68)(419.41, 523.00)(380.30, 556.97)<br>(262.54, 634.76)(328.38, 479.62)(237.04, 589.69)<br>(275.55, 617.70)(313.70, 461.67)(246.20, 586.94)<br>(281.44, 625.42)(536.53, 436.57)(261.75, 584.18)<br>(285.46, 632.02)(540.97, 384.55)(288.69, 577.23)<br>(337.07, 613.78)(537.71, 390.33)(295.47, 577.16)<br>(522.73, 562.93)(545.68, 385.28)(293.91, 574.95)<br>(522.96, 563.79)(545.40, 376.27)(280.46, 577.27)<br>(523.40, 563.31)(557.39, 376.70)(272.27, 578.91)<br>(529.10, 559.97)(557.09, 371.17)(260.93, 580.23)<br>(525.56, 563.93)(567.31, 385.76)(244.85, 580.93)<br>(505.92, 556.01)(563.11, 382.48)(241.06, 582.86)<br>(388.44, 604.90)(543.12, 379.48)(240.99, 582.96)<br>(336.32, 625.65)(333.92, 377.55)(242.76, 582.89)<br>(282.28, 643.31)(292.88, 376.38)(244.60, 582.50)<br>(307.66, 658.97)(290.79, 377.83)(245.23, 581.76)<br>(376.14, 600.09)(293.40, 379.26)(298.70, 572.13)<br>(451.79, 562.19)(300.25, 386.05)(190.72, 552.65)<br>(486.19, 536.60)(302.04, 389.13)(319.01, 543.89)<br>(484.65, 533.05)(315.12, 392.73)(440.73, 542.68)<br>(610.54, 517.17)(345.54, 391.56)(447.97, 554.39)<br>(699.25, 511.48)(402.20, 388.90)(468.50, 544.94)<br>(692.16, 514.77)(428.40, 386.80)(459.98, 530.04)<br>(506.62, 549.65)(427.86, 387.21)(471.14, 533.25)<br>(477.21, 558.69)(431.91, 387.21)(596.86, 526.35)<br>(498.50, 554.03)(432.71, 386.84)(679.44, 514.44)<br>(687.44, 517.82)(430.12, 387.03)(613.08, 522.53)<br>(696.06, 516.64)(427.60, 387.68)(652.80, 516.61)<br>(710.15, 518.46)(425.93, 388.15)(672.16, 510.48)<br>(683.56, 512.31)(430.54, 388.02)(621.88, 522.22)<br>(606.25, 522.94)(432.69, 385.33)(488.20, 539.20)<br>(588.09, 508.86)(484.58, 377.94)(496.53, 536.97)<br>(621.81, 508.74)(524.89, 376.85)(520.19, 534.68)<br>(538.31, 501.98)(548.53, 364.81)(564.83, 532.61)<br>(336.23, 521.72)(528.93, 386.79)(527.86, 540.72)<br>(296.59, 505.43)(479.80, 409.39)(499.38, 542.90)<br>(292.45, 524.79)(400.06, 453.70)(465.89, 547.74)<br>(308.48, 518.94)(166.38, 563.46)(462.86, 545.84)<br>(310.28, 531.24)(103.58, 633.66)(472.87, 543.01)<br>(516.88, 515.13)(122.64, 591.53)<br><img src="/images/10/1-8.png" alt="图 8 涉及到的一些数据处理代码"></p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h4 id="一、SteamVR安装步骤及其操作"><a href="#一、SteamVR安装步骤及其操作" class="headerlink" title="一、SteamVR安装步骤及其操作"></a>一、SteamVR安装步骤及其操作</h4><p>一、下载Steam’<br>打开官网：<a href="https://store.steampowered.com/">https://store.steampowered.com/</a><br>根据指引下载Steam安装包<br><img src="/images/10/2-1.png"><br><img src="/images/10/2-2.png"><br><img src="/images/10/2-3.png"><br>下载完成后进入Steam。注册一个Steam账号并登入<br><img src="/images/10/2-4.png"><br>二、下载SteamVR<br>登入账号后点击库，在查找中搜索SteamVR。<br><img src="/images/10/2-5.png"><br>选中SteamVR按鼠标右键，选择安装游戏。<br><img src="/images/10/2-6.png"><br>安装成功后启动SteamVR<br><img src="/images/10/2-7.png"><br>三、使用SteamVR调试设备<br>首先，按照下述步骤将VR头盔与PC连接上，然后确保基站正常启动，最后打开手柄电源。<br>（一）HTC Vive设备安装<br>1.安装支架（Vive自带的支架需要在墙上打孔；淘宝购买的可以直接打开使用）<br><img src="/images/10/2-8.png"><br>2.安装基站<br><img src="/images/10/2-9.png"><br>3.调试基站<br><img src="/images/10/2-10.png"><br>4.连接设备</p><h4 id="二、Unity操作及其系统学习"><a href="#二、Unity操作及其系统学习" class="headerlink" title="二、Unity操作及其系统学习"></a>二、Unity操作及其系统学习</h4><p>第一阶段：Unity初步认识<br>什么是Unity<br><img src="/images/10/2-11.png"><br>Unity是一个游戏引擎，也就是所谓做游戏的一个工具，市面上有很多做游戏的引擎，Unity就是其中一个。只不过Unity所占的市场份额比较大，市面上大部分的手游都是出自Unity引擎，包括很火的王者荣耀，炉石传说等等。下面是unity的窗口介绍。<br><img src="/images/10/2-12.png"></p><p>第二阶段：C#开发语言相关知识学习<br>因为Unity中的脚本使用C#语言开发，所以学习C# 应该才是学习Unity的第一步。</p><p>第三阶段：Unity引擎学习<br>Unity引擎 的界面布局介绍和使用<br>在了解完什么是Unity之后，我们明白Unity就是一个可视化的工具引擎，我们需要利用这个工具来创造出一系列的作品。学会菜单栏、六大视图等等的使用，才能好好的利用Unity这个工具。<br>unity基础操作:创建项目，创建物体，移动物体，旋转缩放物体，复制物体，坐标<br><img src="/images/10/2-13.png"><br>创建物体，为物体添加颜色 材质和表面贴图<br><img src="/images/10/2-14.png"><br>从网站上下载的模型，导入unity。或者去blender创建模型导入unity，然后更换物体的材质颜色。<br><img src="/images/10/2-15.png"><br>利用Audio Source组件导入音频为模块设置音频。此处是自己录的“人工智能”到声音<br><img src="/images/10/2-16.png"><br>将界面切换成2by3的形式，game窗口和场景窗口并存。同时调整摄像机和光源light。<br><img src="/images/10/2-17.png"><br><img src="/images/10/2-18.png"><br>添加脚本，在vs里编辑代码（帧更新），编译，挂载脚本，在unity里运行。<br><video src="../images/VR1.mp4" controls="controls" width="791">您的浏览器不支持播放该视频！</video><br>在vs里编译一个帧更新+时间和使物体沿x轴移动的代码，加上以前的音频模块，最终形象如视频展示。</p><p>第四阶段：项目架构学习和实战练习<br>Unity常用框架学习<br>学会Unity使用之后，有很多功能实现并不需要我们自己从头到尾去做了。<br>还有数据库框架、Socket框架等。<br>Unity中的AssetBundle学习<br>AssetBundle是用来打包工程中的资源的，一般针对不同类型的资源会打包到不同的包里，这样在更新的时候就可以更方便的下载。<br>游戏项目实战<br>到这一步就是自己制作游戏进行实战练习了<br><video src="../images/VR2.mp4" controls="controls" width="791">您的浏览器不支持播放该视频！</video></p><p>第五阶段：Unity 高级图形学<br>这一阶段就是高级知识了，包括图形学中的各种操作，和底层渲染逻辑等等。</p><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="所需设备"><a href="#所需设备" class="headerlink" title="所需设备"></a>所需设备</h4><p>·Unity: 2019.2.5f1<br>·Steam VR: 1.11.11<br>·SRanipal, a software development kit (SDK) for eye tracking provided by HTC: 1.1.0.1<br>·SR Runtim: 1.1.2.0<br>&lt;Update on 2023-08-18&gt;<br>·Eye and Facial Tracking SDK: 1.3.6.8<br>·SR Runtime: 1.3.6.11<br>·Eye camera: 2.41.0-942e3e<br><img src="/images/10/3-1.png"></p><h4 id="采集数据的步骤"><a href="#采集数据的步骤" class="headerlink" title="采集数据的步骤"></a>采集数据的步骤</h4><p>一、Unity与VR设备集成<br>步骤一：Unity环境搭建<br>首先，根据Unity的官方指导，下载并安装Unity软件。确保软件版本与VR设备兼容，并完成必要的账户注册与验证。<br>步骤二：VR设备连接与配置<br>将VR设备与计算机连接，按照设备说明进行必要的配置和校准，确保设备正常工作并与Unity软件兼容。</p><p>二、眼动追踪插件安装与配置<br>步骤一：插件选择<br>根据所选VR设备的眼动追踪功能，选择合适的眼动追踪插件。常见的插件包括OpenXR和SteamVR等。<br>步骤二：插件安装<br>在Unity中，通过Package Manager安装所选的眼动追踪插件。确保安装过程中选择与VR设备和Unity版本相匹配的插件版本。<br>步骤三：插件配置<br>根据插件的文档和指南，进行必要的配置和初始化操作，以确保眼动追踪功能在Unity中正常工作。</p><p>虚拟现实环境通过链接盒输出到VIVE Pro Eye头戴式耳机实现。嵌入在VR头显中的眼动仪记录眼球运动，测量数据存储在计算机存储器中。安装好基站是配置设备中至关重要的一步。下图介绍了VIVE Pro Eye的安装过程。<br><img src="/images/10/3-2.png"></p><p>三、眼动数据采集<br>步骤一：场景搭建与任务设计<br>在Unity中创建或导入VR场景，并设计相应的实验任务。任务应考虑到用户的视觉行为和交互需求，以获取有效的眼动数据。<br>步骤二：数据采集脚本编写<br>编写Unity脚本，用于获取眼动追踪插件提供的眼动数据。这包括注视点坐标、注视时长、扫描路径等关键指标。<br>步骤三：数据采集与记录<br>运行VR场景，让用户进行任务操作，同时记录眼动数据。确保数据采集过程中设备的稳定性和准确性。</p><p>我们利用归一化的简化计算的方式，建立了 VIVE Pro Eye 的坐标系。我们目前已采集300张VR眼动图片数据，以图1及下列坐标为例，我们采集的图片和数据坐标情况如下：<br><img src="/images/10/3-3.png"><br><img src="/images/10/3-4.png"></p><h4 id="应用unity制作小游戏"><a href="#应用unity制作小游戏" class="headerlink" title="应用unity制作小游戏"></a>应用unity制作小游戏</h4><p>最后，我们利用unity平台，制作了一个简易的“小球滚动”游戏。在小球不断向前滚动的同时，玩家可以通过操纵小球进行简单的左右移动以躲避路上的障碍物。当触及障碍物的时候，会提醒游戏结束，玩家可以按“R”键重新开始。同样，如果玩家操纵小球掉下边缘，也会触发游戏结束。<br><img src="/images/10/3-5.png"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基于VR设备的眼动数据采集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Deep Reinforcement Learning Approach to Supply Chain Inventory Management</title>
      <link href="/2023/10/21/GroupEleven/"/>
      <url>/2023/10/21/GroupEleven/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>A Deep Reinforcement Learning Approach to Supply Chain Inventory Management<br>基于深度强化学习的解决库存管理和价格优化问题</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>此项目是基于深度强化学习的解决库存管理和价格优化问题</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>邓方昱 李秋洁 李蕊</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><p>此项目是基于深度强化学习的解决库存管理和价格优化问题</p><p>论文中先讨论了如何对传统价格优化问题进行修改以增加优化的复杂性</p><p>接下来在简单环境中通过DQN算法进行价格优化 通过DDPG进行多级库存优化</p><p>然后讨论了如何使用RLlib（基于深度强化学习的开源库）简化代码并使其更健壮</p><p>最后开发一个更复杂的环境（一个工厂，仓库和运输）并在其中进行深度强化学习</p><p>我们组了解了深度强化学习与深度学习的具体差别：它介于完全监督和完全没有预定义标签之间，会用到很多已经比较完善的监督学习方法来学习数据的表示（比如用深度神经网络来进行函数逼近、随机梯度下降和反向传播）它也不像非监督学习那样完全不需要其他信息，而是需要奖励系统（通过观察奖励并将其与选择的动作关联起来，智能体将学习如何更好地选择动作）</p><p>同时了解了深度强化学习涉及的几个概念：智能体 环境 动作 奖励 观察</p><p>接下来需要将论文以及代码看懂 并学会如何通过RLlib对代码进行优化</p><h3 id="第八周-及-第十二周"><a href="#第八周-及-第十二周" class="headerlink" title="第八周 及 第十二周"></a>第八周 及 第十二周</h3><h4 id="1-软件安装"><a href="#1-软件安装" class="headerlink" title="1.软件安装"></a>1.软件安装</h4><p>用conda指令安装pytorch:<br>先用conda指令创建一个环境命名pytorch<br><img src="/images/11/2-1.png"><br>进入创建的屋子<br><img src="/images/11/2-2.png"><br>去官网复制pytorch安装的指令<br><img src="/images/11/2-3.png"><br>开始安装pytorch<br><img src="/images/11/2-4.png"><br>Pip list查看<br><img src="/images/11/2-5.png"><br>导入torch检验是否安装成功<br><img src="/images/11/2-6.png"><br>返回true安装成功<br>接下来安装项目用到的包:gym,ray,math,numpy,pandas,matplotlib,seaborn<br>可以用pip语句也可以直接在pycharm中安装<br><img src="/images/11/2-7.png"></p><h4 id="2-代码调试"><a href="#2-代码调试" class="headerlink" title="2.代码调试"></a>2.代码调试</h4><p>(1)<br><img src="/images/11/2-8.png"><br>Matplotlib 提供的seaborn 样式自3.6起已弃用，因为它们不再对应于seaborn 提供的样式。但是，它们仍将以“seaborn-v0_8-＜style﹥”形式提供。或者，直接使用seaborn API。plt.style.use(‘seaborn-white’)<br>修改:(‘seaborn-white’)改成(‘seaborn-v0_8-white’)<br><img src="/images/11/2-9.png"><br>(2)<br><img src="/images/11/2-10.png"><br>只有数据没有图<br>修改:在后面加上一行plt.show()<br><img src="/images/11/2-11.png"><br>(3)<br><img src="/images/11/2-12.png"><br>预计所有张量都在同一设备上，但发现至少有两个设备，cuda:0 和 cpu！（在方法wrapper_CUDA_addmm中检查参数mat1的参数时）</p><p>张量:表示高维数组<br>最开始读取数据时的tensor变量copy一份到device所指定的GPU上去，之后的运算都在GPU上进行<br>在做高维特征运算的时候，采用GPU无疑是比用CPU效率更高，如果两个数据中一个加了.cuda()或者.to(device)，而另外一个没有加，就会造成类型不匹配而报错代码运行结果<br><img src="/images/11/2-13.png"><br>修改:<br>将Tensor或模型移动到指定的设备上：tensor.to(‘cuda:0’)</p><ol><li>Tensor.to(device)<br><img src="/images/11/2-14.png"><br><img src="/images/11/2-15.png"><br><img src="/images/11/2-16.png"></li><li>model.to(device)<br><img src="/images/11/2-17.png"><br><img src="/images/11/2-18.png"></li></ol><h4 id="3-代码运行"><a href="#3-代码运行" class="headerlink" title="3.代码运行"></a>3.代码运行</h4><p><img src="/images/11/2-19.png"><br><img src="/images/11/2-20.png"><br><img src="/images/11/2-21.png"><br><img src="/images/11/2-22.png"><br><img src="/images/11/2-23.png"><br><img src="/images/11/2-24.png"><br><img src="/images/11/2-25.png"></p><h4 id="4-框架理解"><a href="#4-框架理解" class="headerlink" title="4.框架理解"></a>4.框架理解</h4><p>传统定价模型是用某种需求模型对不同定价方案进行假设,用线性规划进行优化,<br><img src="/images/11/2-26.png"><br>目标函数使利润达到最大(给定价格水平×相应需求)<br>约束条件:确保每个时间间隔只有一个价格,因此引入0 1变量x,<br>第二个约束确保需求总和达到库存水平</p><p>接下来构建需求模型:由于需求会受近期价格影响(价格下降会使需求激增,价格上涨会使需求下降),且价格上涨的影响和价格下降的影响不对称<br>构建价格-需求函数:<br><img src="/images/11/2-27.png"><br>由于价格与需求成反比得出第一项的线性关系,对于价格上涨和价格下降分别定义a b两个不同敏感性系数,s冲击函数指定价格与需求的非线性关系<br>用几个时间步长的价格向量得出相应利润函数,使价格时间表恒定得出的不是最优的定价,因此通过贪心算法动态寻找最优定价,即在每个时间步下寻找最优价格,冻结上一步并继续寻找下一时间步的最优价格<br>通过对价格-需求函数内部简单时间依赖关系的分析,可以知道利润不仅取决于当前价格还取决于价格的动态,因此可以采用更为灵活的方法–深度强化学习:<br>状态:任意时间步长的状态为所有先前时间步长的价格向量<br><img src="/images/11/2-28.png"><br>奖励:最大化t时间步长期间的累积奖励<br><img src="/images/11/2-29.png"><br>策略:可以编写递归方程<br><img src="/images/11/2-30.png"><br>γ代表先前状态价值的影响,(如果环境范围无限,γ=1则所有状态价值都为无穷大,环境范围有限则γ可以为1)<br>用时间差分误差作为损失函数逼近Q函数<br><img src="/images/11/2-31.png"><br>操作:以最大Q值执行操作<br>调试:改变γ的值<br>γ=1相对平坦不易区分<br>Γ=0.8只关注前10-12步有助于更快学习</p><h4 id="5-自我理解"><a href="#5-自我理解" class="headerlink" title="5.自我理解"></a>5.自我理解</h4><p>论文中使用的是一步的DQN$Q^{/pi}(s,a)=r+γmaxQ(s^{‘},a^{‘})$<br>可以递归表示出Q(st+1, at+1)<br><img src="/images/11/2-32.png"><br>a,t+1意味着在t+1时刻执行动作a后的立即奖励。如果假设t+1时刻选择的动作a是最优的或接近最优的，可以省去max a操作，得到：<br><img src="/images/11/2-33.png"><br>这个值可以一次又一次的展开,通过将1步状态转移替换成更长的n步状态转移序列，就可以将这种展开应用到DQN更新中<br>假设有一个简单的4个状态（s1, s2, s3, s4）的环境，并且除了终结状态s4，每个状态都只有一个可用的动作<br><img src="/images/11/2-34.png"><br>在一步的情况下<br>1）Q(s1, a)←r1+γQ(s2, a)。<br>2）Q(s2, a)←r2+γQ(s3, a)。<br>3）Q(s3, a)←r3。<br>在训练开始时，顺序地完成前面的更新。前两个更新是没有用的，因为当前Q(s2, a)和Q(s2, a)是不对的，并且只包含初始的随机值。唯一有用的更新是第3个更新，它将奖励r3正确地赋给终结状态前的状态s3。<br>在第2次迭代，正确的值被赋给了Q(s2, a)，但是Q(s1, a)的更新还是不对的。只有在第3次迭代时才能给所有的Q赋上正确的值。所以，即使在1步的情况下，它也需要3步才能将正确的值传播给所有的状态。<br>在2步的情况下<br>1）Q(s1, a)←r1+γr2+γ2Q(s3, a)。<br>2）Q(s2, a)←r2+γr3。<br>3）Q(s3, a)←r3。<br>在这种情况下，第一次更新迭代会将正确的值赋给Q(s2, a)和Q(s3, a)。第二次迭代，Q(s1, a)也会被正确地更新。所以多步可以提升值的传播速度，也就是会加速收敛。<br>因此可以通过n步DQN提升收敛速度</p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 供应链库存管理的深度强化学习方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using neural networks to solve differential equation</title>
      <link href="/2023/10/20/GroupTwelve/"/>
      <url>/2023/10/20/GroupTwelve/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Using neural networks to solve differential equation<br>利用神经网络求解微分方程</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>利用神经网络求解微分方程</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>陈敏华 司笑雨</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><p>偏微分方程实例如下：<br><img src="/images/12/1-1.png"></p><p>考虑以下边界条件：<br><img src="/images/12/1-2.png"></p><p>求解思路：<br>1、设定基础参数<br>2、通过设置函数计算不同位置(中心位置和边界位置)的函数值<br>3、建立神经网络(MPL网络)<br>4、利用神经网络和已知的函数值建立误差函数，用于获得误差<br>5、调用pytorch函数进行训练迭代<br>6、查看结果</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><p>1、定义随机数<br><img src="/images/12/3-1.png"></p><p>2.定义计算函数<br><img src="/images/12/3-2.png"><br><img src="/images/12/3-3.png"></p><p>3.神经网络的构建<br><img src="/images/12/3-4.png"></p><p>4.对损失函数的定义<br><img src="/images/12/3-5.png"><br>接下来是对损失的计算，先对内点进行损失计算，其他的是在边界上，最后的数据和解析解的结果进行对比，求得误差<br><img src="/images/12/3-6.png"><br>接下来是训练过程，然后进行循环，这个循环结束之后就完成了神经网络的训练<br><img src="/images/12/3-7.png"><br>接下来的这些都是用来看结果的，产生一些计算区域内的点，利用这些点来看实际结果和利用神经网络预测结果的区别<br><img src="/images/12/3-8.png"><br>下面的就是把图画出来<br><img src="/images/12/3-9.png"><br>首先是通过PINN得到的结果<br><img src="/images/12/3-10.png"><br>然后是通过实际解析解得到的结果<br><img src="/images/12/3-11.png"><br>最后是误差 可以看到误差是非常小的<br><img src="/images/12/3-12.png"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 利用神经网络求解微分方程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DDPM</title>
      <link href="/2023/10/19/GroupThirteen/"/>
      <url>/2023/10/19/GroupThirteen/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>DDPM<br>扩散模型</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>扩散模型是一种生成模型，实现从噪声（采样自标准正态分布）生成目标数据样本。 </p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>张菁芝 秦越 邹睿</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="一、原理介绍："><a href="#一、原理介绍：" class="headerlink" title="一、原理介绍："></a>一、原理介绍：</h4><p>扩散模型是一种生成模型，实现从噪声（采样自标准正态分布）生成目标数据样本。<br>扩散模型包括两个过程：前向过程（forward process）和反向过程（reverse process）。<br><img src="/images/13/1-1.png"><br>前向过程也就是加噪过程，即上图中的到的过程，我们向原始图像中逐步添加高斯噪声，并且后一时刻都是由前一时刻添加噪声得到的，这样我们就得$x_1x_2······x_{t-1}x_T$，其中$x_T$就是一个完全的高斯噪声。前向过程存在的意义就是帮助神经网络去训练逆向过程，也即前向过程中得到的噪声就是一系列标签，根据这些标签，逆向过程在去噪的时候就知道噪音是怎么加进去的，进而进行训练，正向过程对应网络的训练过程。<br>反向过程也就是去噪过程，即上图中到的过程。我们从标准正态分布采样的高斯噪声$x_T$逐步对其去噪，得到是没有噪声的的图像。逆向过程对应网络的推理过程。</p><h4 id="二、公式推导："><a href="#二、公式推导：" class="headerlink" title="二、公式推导："></a>二、公式推导：</h4><p>1.前向过程：<br>加噪过程主要符合以下公式：<br>$x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}z_1$<br>其中\sqrt{\alpha_t}是预先设定好的超参数<br>通过迭代推导<br>$x_{t-1} = \sqrt{\alpha_{t-1}}x_{t-2} + \sqrt{1-\alpha_{t-1}}z_2$<br>$x_t = \sqrt{\alpha_t}(\sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}z_1) + \sqrt{1-\alpha_t}z_1$<br>     $= \sqrt{\alpha_t\alpha_{t-1}}x_{t-2} + (\sqrt{\alpha_t(1-\alpha_{t-1})}z_2 + \sqrt{1-\alpha_t}z_1)$<br>其中，$z_1$,$z_2$都服从高斯分布，分别为：<br>N（0，$1-\alpha_t$）<br>N（0，$\alpha_t(1-\alpha_{t-1})$)<br>由N（0，$\sigma_1^2I$）+N（0，$\sigma_2^2I$）~N（0，$(\sigma_1^2 + \sigma_2^2)I$）得，相加后仍服从高斯<br>$x_t = \sqrt{\overline{\alpha_t}}x_0 + \sqrt{1-\overline{\alpha_t}}z_t$<br>其中$\overline{\alpha_t}=\prod_i^t\alpha_i$，这是随Noise schedule设定好的超参数，$z_{t-1}∼N(0,1)$也是一个高斯噪声。通过上述两个公式，我们可以不断的将图片进行破坏加噪<br><img src="/images/13/1-2.png"><br><img src="/images/13/1-3.png"><br>我们通过往图片中加入噪声，使得图片变得模糊起来，当加的步骤足够多的时候，图片会非常接近一张纯噪声。纯噪声也就意味着多样性，我们的模型在去噪的过程中能够产生更加多样的图片。<br>2.反向过程：<br>反向过程就是通过估测噪声，多次迭代，逐渐将被破坏的$x_t$恢复成$x_0$。<br>根据贝叶斯公式，已知$x_t$反推$x_{t-1}$：<br>$q(x_{t-1}|x_t,x_0) = q(x_t|x_{t-1},x_0)$${q(x_{t-1}|x_0)}\over{q(x_t|x_0)}$</p><p>${q(x_{t-1}|x_0)} = x_t = \sqrt{\overline\alpha_{t-1}}x_0 + \sqrt{1-\overline{\alpha_{t-1}}}z$</p><p>${q(x_t|x_0)} = x_t = \sqrt{\overline\alpha_t}x_0 + \sqrt{1-\overline{\alpha_t}}z$</p><p>$q(x_{t-1}|x_t,x_0) = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}z $<br>最终得到：<br>$x_{t-1} = $$1\over\alpha_t$$(x_t-$${1-\alpha_t}\over{\sqrt{1-\overline\alpha_t}}$$\epsilon_{\theta}(x_t,t) + {\sigma}_tz$<br>由于加噪过程中的真实噪声在复原过程中是无法获得的，因此DDPM的关键就是训练一个由$x_t$和$t$估测噪声的模型$\epsilon_0(x_t,t)$,其中$\theta$就是模型的训练参数，${\sigma}_t$也是一个服从标准正态分布的高斯噪声，用于表示估测与实际的差距。在DDPM中，使用U-Net作为估测噪声的模型。<br>本质上，就是训练这个U-net模型，该模型输入为$x_t$和t，输出为时刻的高斯噪声，即利用$x_t$和t预测这一时刻的高斯噪声，这样就可以一步一步的再从噪声回到真实图像。</p><h4 id="三、训练和推理"><a href="#三、训练和推理" class="headerlink" title="三、训练和推理"></a>三、训练和推理</h4><p>模型结构：U-Net<br>模型输入：$x_t$、t<br>模型输出：噪声<br>模型学习目标：不断逆向去掉噪声，让每步逆向预测的噪声$\epsilon_0(x_t,t)$和每步前向添加的噪声尽可能相近$\epsilon$<br>损失函数：<br>Loss=$||{\epsilon-\epsilon_{\theta}(x_t,t)}^2|| = ||{\epsilon-\epsilon_{\theta}(\sqrt{\overline\alpha_t}x_{t-1}+\sqrt{1-\overline\alpha_t}\epsilon,t)}^2||$<br>训练过程：最小化预测噪声和添加的噪声的差距<br>第一步：输入原图$x_0$<br>第二步：生成随机噪声和时间t<br>第三步：对原图加噪，<br>第四步：将加噪的图像和时间向量t输入神经网络模型，预测噪声<br>第五步：计算loss并更新模型<br>推理的过程：<br>从N（0~1）中随机生成一个噪声<br>循环T步逐步去噪，就从噪声恢复得到了原图<br><img src="/images/13/1-4.png"><br><img src="/images/13/1-5.png"><br><img src="/images/13/1-6.png"><br><img src="/images/13/1-7.png"></p><h4 id="四、代码复现"><a href="#四、代码复现" class="headerlink" title="四、代码复现"></a>四、代码复现</h4><p><img src="/images/13/1-8.png"><br><img src="/images/13/1-9.png"><br><img src="/images/13/1-10.png"><br><img src="/images/13/1-11.png"></p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h4 id="一、Model"><a href="#一、Model" class="headerlink" title="一、Model"></a>一、Model</h4><p>·时间嵌入<br>·上下采样块<br>·注意力机制<br>·残差块<br>·UNet<br>步骤：<br>输入数据首先经过一个卷积层，用于提取初始特征。<br>通过一系列的下采样操作，即ResBlock和DownSample模块，逐步减小特征图的空间尺寸，并增加特征的抽象程度。下采样过程中，特征图的通道数逐步增加，以便更好地捕获不同层次的特征信息。<br>中间处理，经过一系列的ResBlock，对下采样得到的特征进行进一步处理，以提高抽象特征的表示能力。<br>通过一系列的上采样操作，即ResBlock和UpSample模块，将特征图逐步还原到原始输入的空间尺寸。在上采样的过程中，利用跳跃连接将对应的下采样阶段的特征图与当前阶段的特征图进行拼接，从而融合不同分辨率的特征信息。<br>输出预测，对上采样得到的特征图进行最终的特征提取和输出预测。<br>实际上是通过将最终的特征图映射到3个通道，得到最终的噪声预测结果。<br><img src="/images/13/2-1.png"><br><img src="/images/13/2-2.png"><br><img src="/images/13/2-3.png"><br><img src="/images/13/2-4.png"></p><h4 id="二、训练"><a href="#二、训练" class="headerlink" title="二、训练"></a>二、训练</h4><p><img src="/images/13/2-5.png"></p><h4 id="三、采样"><a href="#三、采样" class="headerlink" title="三、采样"></a>三、采样</h4><p><img src="/images/13/2-6.png"><br><img src="/images/13/2-7.png"></p><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="一、理论学习"><a href="#一、理论学习" class="headerlink" title="一、理论学习"></a>一、理论学习</h4><p>背景：扩散模型最初由艺术家Tero Karras等人在2020年提出。这种模型的灵感源自物理学中的扩散过程。其基本思想是通过多次迭代扩散噪声来生成图像。</p><p>在该模型提出之前：<br>变分自编码器（Variational Autoencoders，VAE）：VAE是一种生成模型，由编码器和解码器组成。编码器将输入图像编码为潜在空间中的分布参数，解码器将这些参数解码为图像。VAE通过最大化生成图像的似然性来训练。<br>生成对抗网络（Generative Adversarial Networks，GAN）：GAN由生成器和判别器组成。生成器试图生成逼真的图像，而判别器试图区分生成的图像和真实图像。GAN的训练过程是一个对抗性的博弈过程，最终目标是使生成的图像难以被判别器区分。<br><img src="/images/13/3-1.png"><br>扩散模型包括两个过程：前向过程和逆向过程，其中前向过程又称为扩散过程，无论是前向过程还是逆向过程都是一个参数化的马尔可夫链。其中逆向过程可以用来生成数据<br><img src="/images/13/3-2.png"></p><p>训练过程——前向过程<br>采样过程——逆向过程<br><img src="/images/13/3-3.png"></p><h4 id="二、公式推导"><a href="#二、公式推导" class="headerlink" title="二、公式推导"></a>二、公式推导</h4><p>需要用到的知识：<br>1.一般的条件概率形式<br>2.马尔科夫链条件概率形式<br>3.先验概率和后验概率<br>4.重参数化技巧<br><img src="/images/13/3-4.png" alt="前向过程："><br><img src="/images/13/3-5.png" alt="逆向过程："></p><h4 id="三、代码实现"><a href="#三、代码实现" class="headerlink" title="三、代码实现"></a>三、代码实现</h4><p>环境：Python 3.11  深度学习框架：Pytorch<br>MNIST数据集——手写数字生成<br>epoch:10<br>T=500<br>batch size=64<br>学习率：0.0005<br><img src="/images/13/3-6.png"><br><img src="/images/13/3-7.png"></p><p>107张人脸图片——人脸生成<br>epoch:1000<br>T=1000<br>batch size=80<br>学习率：0.0001<br><img src="/images/13/3-8.png"></p><h4 id="四、深入理解"><a href="#四、深入理解" class="headerlink" title="四、深入理解"></a>四、深入理解</h4><p>·扩散过程是一个马尔科夫过程，即当前时刻的状态转移概率只与前一时刻有关，每个时间步都是前一个时间步的演变结果，也就是说每个时间步都保留了前一个时间步的信息。而各个时间步中图像的分布聚集在一起可以看作是一个潜在的特征空间。整个前向过程在做的事情就是，通过在每一时刻向图像加入噪声，从而不断地将图像的特征在时间序列上进行拆分，时间序列越长，就会有更多的特征被拆解，而每一步拆解的过程模型是可以学习的（这会帮助模型在之后的采样过程中重建出图像）。当t足够大时，最后得到的将是一张纯高斯噪声，此时整个过程就可以理解为将原始数据中的复杂成分，通过一系列逐步的扩散转换为一个简单的先验分布，而这个分布的均值和方差是已知的。</p><p>·扩散过程具有时间依赖性：<br>x_t=√α ̅_tx_0+√1−α ̅_tz_t<br>参数α ̅_t可以被视为在不同时间步上控制扩散过程的强度的参数，随着时间步的增加，α ̅_t的变化会影响数据生成的过程。整个过程可以理解为将原始数据x_0中的复杂成分，通过一系列逐步的扩散转换为一个简单的先验分布，而这个分布的均值和方差是已知的。</p><p>·采样过程：<br>只有当t&gt;1时才需要采样一个噪声z，因为如果t＝1，这时计算的是x_0，我们需要得到的是一张没有噪声的图像；加入噪声z的目的是为了模拟一些不确定性和随机性。在公式x_t−11/√α_t（x_t−1−α_t/√1−(α_t) ̅ϵ_θ(x_t,t)）σ_tz中，均值为1/√α_t（x_t−1−α_t/√1−(α_t) ̅ϵ_θ(x_t,t)），方差为σ_t，从x_T恢复至x_0的过程，可以理解为均值是给定一个方向，方差是在这个方向上增加一定的扰动性。</p><p>·噪声预测网络选择：<br>扩散模型的网络的输入和输出都是同等规格的，理论上只要网络的输入规格和输出规格一样就可以。<br>在扩散模型中，生成图像的过程是通过多次迭代扩散噪声来完成的，而每次迭代都可以看作是在时间上的一个步骤，因此，Unet的输入不只是一张图片，同时也要将一个时间向量作为输入传送给网络，这个时间向量被用作表示当前时间步的特征，通过这个时间向量，可以告诉模型是在做哪一步的去噪，随着时间步的前移，模型就能够不断学习到一些更细节的特征。</p><h4 id="五、其他"><a href="#五、其他" class="headerlink" title="五、其他"></a>五、其他</h4><p>模型可以改进的地方：<br>1.训练时间长、训练过后图像生成的速度较慢<br>2.可以控制生成图像的特征，让模型有条件地生成图像。</p><p>目前，DDIM介绍了一种在图像质量几乎没有明显下降的情况下加快图像生成的方法。它通过将扩散过程重新定义为非马尔可夫过程来做到这一点。</p><p>MAE与DDPM对比<br>相同之处：MAE随机遮盖图像的一些块相当于DDPM中对图像进行加噪，这两种操作都是对原本图像的完整特征进行了破坏，破坏的力度在MAE中体现为被遮盖住的部分的多少，在DDPM中体现为加噪的力度，而这个加噪的力度随着时间步的增长而加大。<br>MAE：在学习的时候不看被破坏的那部分，只看没有被破坏的部分，从而学习到特征，用学习到的特征来预测破坏部分的特征，从而重构出完整的图像。（从局部特征中预测出被破坏的部分的特征），相当于一个去噪过程。<br>Diffusion Model：看的是被破坏后整张图片的分布，被破坏的过程是要学习的，然后从被破坏的分布恢复至被破坏前的分布。<br>借助DDPM中拆楼—建楼的想法<br>假如有一栋4层高的楼，在MAE和在DDPM中都对一楼和三楼进行破坏。<br>在MAE中，对一楼三楼破坏掉后，只剩下二楼和四楼，根据二楼和四楼的建造结构，一楼和三楼大概是个什么样的结构也就知道了，然后自己挑选用来建造一楼三楼的建筑材料，从而就可以得到一栋完整的楼房。<br>在DDPM中，也是对一楼和三楼进行破坏，但是拆解一楼三楼的过程是可见的，知道了拆楼的过程也就是步骤之后，用被拆后得到的原料再去重建一楼和三楼。<br>两者有相反的意思，在MAE中，重建过程是基于二楼四楼的特征，而在DDPM中，就是在原本的一楼和三楼拆剩下的原料中重建。</p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 扩散模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Network image denoising</title>
      <link href="/2023/10/18/GroupFourteen/"/>
      <url>/2023/10/18/GroupFourteen/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>Network image denoising<br>网络图像去噪————基于图像遮蔽方法</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>用掩码训练去除图像噪声</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>刘妍 何怡蓉</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="项目具体情况"><a href="#项目具体情况" class="headerlink" title="项目具体情况:"></a>项目具体情况:</h4><p>论文创新方法解决图像去噪问题，不同于传统的基于模型和基于数据驱动方法，采取图像掩码遮蔽方法去除噪声。屏蔽掉一部分输入像素，然后训练深度网络来完成修复。在论文的方法中，使用核大小为 1 的卷积层作为特征嵌入模块，将 3 通道像素值投影到 C 维特征标记中。其中1 * 1 卷积层确保像素在特征嵌入过程中不会相互影响，便于后续的遮蔽操作。主要从两个方面解决去噪问题，输入掩码和注意力掩码。输入掩码随机屏蔽了第一卷积层嵌入的特征标记，并促进网络在训练过程中修复被屏蔽的信息。当训练与真实测试不同时，网络将会通过增加图片亮度来输出图片。因此，还需引入注意力掩码解决问题，可以通过在自我注意力过程中执行相同的掩码操作来缩小训练和测试之间的差距。</p><h4 id="项目流程图"><a href="#项目流程图" class="headerlink" title="项目流程图:"></a>项目流程图:</h4><p><img src="/images/14/1-1.png"></p><h4 id="项目进度："><a href="#项目进度：" class="headerlink" title="项目进度："></a>项目进度：</h4><p>论文翻译完成并相对了解项目的目的及其使用的方法，目前在翻译代码，通过对论文的结合，初步理解代码的作用，目前在细化分析代码的编写逻辑以及配置相关的库。</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="项目背景"><a href="#项目背景" class="headerlink" title="项目背景"></a>项目背景</h4><p>当前深度去噪网络存在泛化能力差的情况，例如，当训练集噪声类型和测试集噪声类型不一致时，模型的性能会大打折扣。作者认为其原因在于网络倾向于过度拟合训练噪声，而没有学习图像自身的内在结构。为了解决这个问题,作者提出了一种遮盖图像训练的策略（masked image training）。<br><img src="/images/14/3-1.png"><br><img src="/images/14/3-2.png"></p><h4 id="项目内容"><a href="#项目内容" class="headerlink" title="项目内容"></a>项目内容</h4><p><img src="/images/14/3-3.png"><br>整体网络架构基于了Swin Transformer。提出一种masked training的方法来提高模型的泛化能力。主要包含两个方面:</p><p><img src="/images/14/3-4.png"><br>Input Mask<br>    在特征提取之后,会对输入图像进行随机大比例遮盖（input mask）,比如遮盖75%~85%的像素。这将构造一个非常具有挑战性的图像修复问题,迫使模型学习重构被遮盖的图像内容,而不能简单依靠检测并移除噪声模式。</p><p>Attention Mask<br>    在self-attention层也进行类似的随机遮盖。这是为了减轻训练和测试的不一致性。由于input mask只在训练使用,测试时的输入是完整的图像。因此我们使用attention mask可以平衡这一差异。</p><h4 id="对代码的理解"><a href="#对代码的理解" class="headerlink" title="对代码的理解"></a>对代码的理解</h4><p><img src="/images/14/3-5.png"><br>这段代码的目的是准备输入图像以满足模型的尺寸要求，执行模型推断，并裁剪输出以匹配原始输入的大小。</p><p><img src="/images/14/3-6.png"><br>这段代码是图像处理和评估的一部分，主要用于计算超分辨率（Super-Resolution），它使用了峰值信噪比（PSNR）、结构相似性指数（SSIM）和LPIPS（Learned Perceptual Image Patch Similarity）三种不同的指标来评估恢复图像与原始高质量图像之间的相似性。</p><p><img src="/images/14/3-7.png"><br>这段代码是对之前计算得到的性能指标（PSNR、SSIM和LPIPS）进行汇总和平均化，并打印出平均值的结果。</p><p><img src="/images/14/3-8.png"><br>这段代码定义一个名为define_model的函数，这个函数主要目的是根据传入的args参数和全局的opt_net字典来定义并返回一个net模型。如果任务类型是masked_denoising，函数会从全局变量opt_net中获取一系列参数，并使用这些参数来创建一个net模型的实例。</p><p><img src="/images/14/3-9.png"><br>这段代码是define_model函数的一部分，它根据args参数初始化一个用于真实世界图像超分辨率的模型。根据判断是否需要大模型，它将选择不同的配置来创建模型。</p><p><img src="/images/14/3-10.png"><br>这段代码是define_model函数的一部分，为灰度图像、彩色图像去噪任务配置和初始化一个模型的。还定义了一个用于减少JPEG压缩伪影的模型，并从指定路径加载了预训练权重。</p><p><img src="/images/14/3-11.png"><br>setup 函数用于根据不同的任务类型(args.task)设置保存目录、文件夹、边界大小和窗口大小等参数。该函数根据args.task的值来选择不同的代码块来执行，并返回设置好的参数。</p><p><img src="/images/14/3-12.png"><br>这段代码定义了一个名为get_image_pair的函数，该函数根据输入的参数args和路径path，从相应的文件位置加载并预处理图像。它针对不同的任务（args.task）进行不同的操作，以获取高质量（gt）和低质量（lq）的图像对，或者仅获取低质量图像。</p><p><img src="/images/14/3-13.png"><br>这段代码定义了一个函数im2tensor，该函数将一个图像（通常是一个NumPy数组）转换为一个PyTorch张量（Tensor），并且对其进行归一化。<br>定义了一个名为test的函数，该函数这段代码实现了一个图像模型的逐块（tile-by-tile）测试功能。</p><h4 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h4><p><img src="/images/14/3-14.png"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络图像去噪——基于图像遮蔽方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BoMD Bag of Multi-label Descriptors for Noisy Chest X-ray Classification</title>
      <link href="/2023/10/17/GroupFifteen/"/>
      <url>/2023/10/17/GroupFifteen/</url>
      
        <content type="html"><![CDATA[<h2 id="基本信息"><a href="#基本信息" class="headerlink" title="基本信息"></a>基本信息</h2><h3 id="项目名称"><a href="#项目名称" class="headerlink" title="项目名称"></a>项目名称</h3><p>BoMD Bag of Multi-label Descriptors for Noisy Chest X-ray Classification<br>多标签符包胸部X射线分类</p><h3 id="项目简介"><a href="#项目简介" class="headerlink" title="项目简介"></a>项目简介</h3><p>BoMD通过利用来自BERT模型的词嵌入中包含的标签的语义信息来平滑地重新标记嘈杂的多标签医学图像数据集，用其邻域的估计标签分布重新标记有噪声的标签样本。</p><h3 id="小组成员"><a href="#小组成员" class="headerlink" title="小组成员"></a>小组成员</h3><p>孙琦 王博艺</p><hr><h2 id="项目进度"><a href="#项目进度" class="headerlink" title="项目进度"></a>项目进度</h2><h3 id="第四周"><a href="#第四周" class="headerlink" title="第四周"></a>第四周</h3><h4 id="论文的目标"><a href="#论文的目标" class="headerlink" title="论文的目标"></a>论文的目标</h4><p>在文中提出了一种专为有噪声多标签CXR 学习设计的新方法，该方法可检测并平滑地重新标记数据集中的有噪声样本，以用于普通多标签分类器的训练，该方法优化了一组多标签描述符(BoMD)，以提高它们与多标签图像注释语言模型产生的语义描述符的相似性。（词汇袋(Bag of Words, BoW)方法[15,54,55]是一种传统的信息检索技术，用无序单词的直方图表示文档。在计算机视觉中，BoW[9,54]用无序局部视觉描述符的直方图表示图像，这些描述符以无监督的方式从训练图像中学习。采用BoW概念，但不是提取局部视觉描述符(例如SIFT[38])，而是训练DNN用一袋全局视觉描述符来表示每个图像。）在有噪声多标签训练集和干净测试集上的实验表明，论文中的模型在许多CXR多标签分类基准中具有最先进的准确性和鲁棒性，包括论文中提出的系统评估有噪声多标签方法的新基准。</p><h4 id="为什么研究这个"><a href="#为什么研究这个" class="headerlink" title="为什么研究这个"></a>为什么研究这个</h4><p>深度学习方法(DNN)（因为大规模的人工标记的干净数据集，在医学影像分类有很高的正确率，但是人工成本太高）因此，新的医学影像分类问题可能需要依赖从放射报告中提取的机器生成(NLP)的噪声标签。事实上，许多胸部X射线（CXR）分类器是基于带有噪声标签的数据集建模的，但它们的训练过程通常对噪声标签样本不够稳健，导致模型效果不佳。此外，CXR数据集大多是多标签的，因此当前的多类噪声标签学习方法不能轻易地适应。所以提出了一种新的方法，专为噪声多标签CXR学习而设计，该方法可以检测并平滑地重新标记数据集中的噪声样本，以用于常见的多标签分类器的训练。提出的方法优化了一组多标签描述符 (Multi-label Descriptors), 以促进它们与由语言模型从多标签图像注释中生成的语义描述符的相似性。</p><h4 id="如何实现"><a href="#如何实现" class="headerlink" title="如何实现"></a>如何实现</h4><p>第一阶段 特征处理器<br>图像描述(image description)学习阶段，将训练图像转换为一组视觉描述符，这些描述符位于由图像标签计算得到的词嵌入所构成的语义空间。<br>第二阶段<br>图构建阶段，用于平滑地重新标记带有噪声的多标签图像，其中每个图像由从学习得到的视觉描述符组成的子图表示，该子图可以捕捉图像之间的细粒度关系。然后，将这个平滑重新标记(smoothly re-labelled)的数据集用于训练多标签分类器。&nbsp;<br>新颖的两阶段学习方法，可平滑地重新标记有噪声的多标签 CXR 图像数据集，然后可用于训练通用的多标签分类器<br>新的多标签图像描述符袋学习方法，可利用语言模型中的语义信息来表示多标签图像并检测噪声样本。<br>新的图结构，用于平滑地重新标记有噪声的多标签图像，每幅图像都由学习到的多标签图像描述符的子图来表示，这种子图可以捕捉细粒度的图像关系。<br>首次对结合了 PadChest 和 Chest Xray 14数据集的噪声多标签方法进行了系统评估。<br><img src="/images/15/1-1.png"><br>BoMD的特征提取器每个图像返回一个描述符v, D是噪声训练集，C是干净集，而~ D是重新标记的训练集。BoMD有两个组成部分：<br>(1)学习一组多标签图像描述符(MID) {v1, v2, v3}来表示图像<br>(2)基于MID描述符之间的细粒度关系，由图结构驱动的图像平滑重新标记。</p><h3 id="第八周"><a href="#第八周" class="headerlink" title="第八周"></a>第八周</h3><h4 id="Python爬虫"><a href="#Python爬虫" class="headerlink" title="Python爬虫"></a>Python爬虫</h4><p>我们爬取的数据是不同省市的大学对不同省份的学生进行录取的一个数据提取。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">from</span> selenium<span class="token punctuation">.</span>webdriver<span class="token punctuation">.</span>common<span class="token punctuation">.</span>by <span class="token keyword">import</span> By<span class="token keyword">from</span> selenium<span class="token punctuation">.</span>webdriver<span class="token punctuation">.</span>common<span class="token punctuation">.</span>keys <span class="token keyword">import</span> Keys<span class="token keyword">from</span> selenium<span class="token punctuation">.</span>webdriver<span class="token punctuation">.</span>support<span class="token punctuation">.</span>ui <span class="token keyword">import</span> WebDriverWait<span class="token keyword">from</span> selenium<span class="token punctuation">.</span>webdriver<span class="token punctuation">.</span>support <span class="token keyword">import</span> expected_conditions <span class="token keyword">as</span> EC<span class="token keyword">from</span> bs4 <span class="token keyword">import</span> BeautifulSoup<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> time<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>导入所需要的一些包，配置环境<br>Pandas它提供了一个简单、高效、带有默认标签（也可以自定义标签）的 DataFrame 对象。<br>能够快速得从不同格式的文件中加载数据（比如 Excel、CSV 、SQL文件），然后将其转换为可处理的对象；<br>能够按数据的行、列标签进行分组，并对分组后的对象执行聚合和转换操作；<br>能够很方便地实现数据归一化操作和缺失值处理；<br>能够很方便地对 DataFrame 的数据列进行增加、修改或者删除的操作；<br>能够处理不同格式的数据集，比如矩阵数据、异构数据表、时间序列等；<br>提供了多种处理数据集的方式，比如构建子集、切片、过滤、分组以及重新排序等<br>BeautifulSoup 是 Python 中一个常用的解析 HTML 和 XML 的第三方库，使用它可以方便地从网页中提取数据.<br><img src="/images/15/2-1.png"><img src="/images/15/2-2.png"><br>Webdriver的工作原理<br>Webdriver直接驱动浏览器来模拟一些人的操作，如点击按钮，输入字符串等.而我们的测试代码是通过发送命令给webdriver完成这些操作，虽然不同的浏览器有不同的驱动，但是我们调用的代码API都是一样的<br>原理：：<br>工程师也就是乘客，编写自动化脚本，脚本里的代码会发送请求给浏览器驱动，浏览器驱动就是司机，它来解析这些代码的含义，解析完之后把它们发送给浏览器，浏览器就想当于是车辆，执行浏览器驱动发来的指令，完成工程师想要的操作。<br><img src="/images/15/2-3.png"><br>那么webdriver和浏览器如何通信：<br>1、对于工程师写的脚本，一个http请求会被创建并且发送给浏览器驱动<br>2、浏览器驱动里面包含了一个HTTP server，用来接收这些请求<br>3、HTTP server接收到这些请求后根据请求来操控对应的浏览器<br>4、浏览器就会执行具体的测步骤<br>5、浏览器将步骤执行的结果返回给HTTP server<br>6、HTTP server又将结果返回给脚本，然后工程师就可以看到最后的操作结果，如果是错误的http代码我们在控制台可以看到错误的信息。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">arrs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'北京'</span><span class="token punctuation">,</span> <span class="token string">'上海'</span><span class="token punctuation">,</span> <span class="token string">'天津'</span><span class="token punctuation">]</span> <span class="token comment"># 初始化身份，暂写三个</span><span class="token comment"># 等待页面元素加载完成</span><span class="token comment"># 初始化Selenium WebDriver，确保下载对应浏览器的驱动程序，模拟浏览器操作，从而获取数据进行爬取</span>driver <span class="token operator">=</span> webdriver<span class="token punctuation">.</span>Chrome<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment"># 模拟用户交互，例如填写搜索框</span><span class="token comment"># 访问目标网页</span>driver<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"https://www.gkzy.com/zytb/score_search/zyfs.html"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/15/2-5.png"><br><img src="/images/15/2-6.png"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> arr <span class="token operator">==</span> <span class="token string">'北京'</span><span class="token punctuation">:</span>    <span class="token comment"># 模拟浏览器点击A-安徽</span>    submit_button <span class="token operator">=</span> wait<span class="token punctuation">.</span>until<span class="token punctuation">(</span>EC<span class="token punctuation">.</span>element_to_be_clickable<span class="token punctuation">(</span>        <span class="token punctuation">(</span>By<span class="token punctuation">.</span>XPATH<span class="token punctuation">,</span> <span class="token string">'//*[@id="myModalLocation"]/div[2]/div/div[2]/table/tbody/tr[1]/td[1]/div/a'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 按下按键</span>    submit_button<span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 等待三秒，等浏览器按键完成</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token comment"># 模拟浏览器点击北京</span>    submit_button <span class="token operator">=</span> wait<span class="token punctuation">.</span>until<span class="token punctuation">(</span>EC<span class="token punctuation">.</span>element_to_be_clickable<span class="token punctuation">(</span><span class="token punctuation">(</span>By<span class="token punctuation">.</span>XPATH<span class="token punctuation">,</span> <span class="token string">'//*[@id="a1"]/ul/li[1]/a'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># 按下按键</span>    submit_button<span class="token punctuation">.</span>click<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment"># 等待三秒，等浏览器按键完成</span>    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这段代码使用了 Selenium 库来模拟网页的操作：</p><p>1.wait.until(EC.element_to_be_clickable((By.XPATH, ‘//<em>[@id=”a4”]/ul/li[2]/a’)))：这部分代码使用 WebDriverWait 对象等待页面元素变为可点击状态。具体来说：<br>2.EC.element_to_be_clickable 表示等待直到元素可点击。<br>3.By.XPATH 指定使用 XPath 表达式来定位元素。<br>4.’//</em>[@id=”a4”]/ul/li[2]/a’ 是一个 XPath 表达式，用于定位网页中的一个元素。根据给定的 XPath 表达式，这段代码会等待 id 为 “a4” 的元素下的第二个 li 元素中的 a 元素变为可点击状态。<br>5.submit_button.click()：一旦元素变为可点击状态，就会执行 click() 方法，模拟用户点击操作。<br>6.time.sleep(2)：这段代码会让程序暂停 2 秒钟。这可能是为了等待页面加载完成或者其他操作，以确保程序执行的顺利进行。<br>综合起来，这段代码的作用是等待页面中特定的链接元素变为可点击状态，然后模拟用户点击该链接，并在点击后等待 2 秒钟。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># 等待数据加载完成</span>time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>  <span class="token comment"># 根据实际情况调整等待时间</span><span class="token comment"># 获取页面源码</span>html <span class="token operator">=</span> driver<span class="token punctuation">.</span>page_source<span class="token comment"># 解析页面源码</span>soup <span class="token operator">=</span> BeautifulSoup<span class="token punctuation">(</span>html<span class="token punctuation">,</span> <span class="token string">'lxml'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>soup<span class="token punctuation">)</span><span class="token comment"># 提取分数线数据</span><span class="token comment"># 假设数据是以表格形式存在</span>table <span class="token operator">=</span> soup<span class="token punctuation">.</span>find<span class="token punctuation">(</span><span class="token string">'table'</span><span class="token punctuation">,</span> <span class="token punctuation">{</span><span class="token string">'class'</span><span class="token punctuation">:</span> <span class="token string">'fs13 cf scoreTable'</span><span class="token punctuation">}</span><span class="token punctuation">)</span>rows <span class="token operator">=</span> table<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'tr'</span><span class="token punctuation">)</span><span class="token comment"># 存储数据</span>    data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> row <span class="token keyword">in</span> rows<span class="token punctuation">:</span>        cols <span class="token operator">=</span> row<span class="token punctuation">.</span>find_all<span class="token punctuation">(</span><span class="token string">'td'</span><span class="token punctuation">)</span>        cols <span class="token operator">=</span> <span class="token punctuation">[</span>ele<span class="token punctuation">.</span>text<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> ele <span class="token keyword">in</span> cols<span class="token punctuation">]</span>        data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>cols<span class="token punctuation">)</span>    <span class="token comment"># 将数据转换为DataFrame</span>    df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>data<span class="token punctuation">)</span>    <span class="token comment"># 存储DataFrame到Excel</span>    df<span class="token punctuation">.</span>to_excel<span class="token punctuation">(</span>arr <span class="token operator">+</span> <span class="token string">"分数线数据.xlsx"</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>柱形图的绘制<br><img src="/images/15/2-7.png"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token comment"># 读取Excel文件</span>excel_file_path <span class="token operator">=</span> <span class="token string">'北京分数线数据.xlsx'</span>  <span class="token comment"># 替换为你的Excel文件路径</span>data_frame <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_excel<span class="token punctuation">(</span>excel_file_path<span class="token punctuation">)</span>data_frame <span class="token operator">=</span> data_frame<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>data_frame<span class="token punctuation">)</span>column_to_plot<span class="token operator">=</span>data_frame<span class="token comment"># 将第二列转换为整数类型</span>column_to_plot<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> column_to_plot<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>column_to_plot<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>column_to_plot<span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'SimSun'</span><span class="token punctuation">]</span>  <span class="token comment"># Windows 系统中的宋体</span>plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># 正确显示负号</span><span class="token comment"># 生成柱状图</span><span class="token comment"># 绘制条形图，第一列作为x轴（分类），第二列作为y轴（数值）</span>column_to_plot<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>kind<span class="token operator">=</span><span class="token string">'bar'</span><span class="token punctuation">,</span> x<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> y<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'各专业北京录取人数'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>column_to_plot.plot(kind=’bar’, x=3, y=10)：这行代码指示 Pandas 对象 column_to_plot 绘制柱状图。<br>kind=’bar’ 指定了要创建的图表类型为柱状图。<br>x=3 指定了数据框中作为 x 轴的数据列索引或列名。在这里，3 表示数据框中的第四列，因为 Python 使用零索引，所以第四列索引为 3。<br>y=10 指定了数据框中作为 y 轴的数据列索引或列名。在这里，10 表示数据框中的第十一列。<br>这行代码的作用是根据数据框 column_to_plot 中的指定列绘制柱状图，其中第四列作为 x 轴数据，第十一列作为 y 轴数据。<br><img src="/images/15/2-8.png"><br>饼状图的绘制</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token comment"># 1. 加载 Excel 文件并选择第一列和第二列数据</span>df <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_excel<span class="token punctuation">(</span><span class="token string">'workpad.xlsx'</span><span class="token punctuation">,</span> engine<span class="token operator">=</span><span class="token string">'openpyxl'</span><span class="token punctuation">)</span>selected_columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'subject'</span><span class="token punctuation">,</span> <span class="token string">'people'</span><span class="token punctuation">]</span>  <span class="token comment"># 你需要替换为实际的列名</span><span class="token comment"># 2. 绘制饼状图</span>plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 设置图形大小</span>plt<span class="token punctuation">.</span>pie<span class="token punctuation">(</span>df<span class="token punctuation">[</span>selected_columns<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> labels<span class="token operator">=</span>df<span class="token punctuation">[</span>selected_columns<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> autopct<span class="token operator">=</span><span class="token string">'%1.1f%%'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'Pie Chart from Excel Data'</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="/images/15/2-9.png"><br><img src="/images/15/2-10.png"><br>北京<br>天津<br><img src="/images/15/2-11.png"><br><img src="/images/15/2-12.png"><br>上海<br><img src="/images/15/2-13.png"><br><img src="/images/15/2-14.png"></p><h3 id="第十二周"><a href="#第十二周" class="headerlink" title="第十二周"></a>第十二周</h3><h4 id="选题的背景意义"><a href="#选题的背景意义" class="headerlink" title="选题的背景意义"></a>选题的背景意义</h4><p><img src="/images/15/3-1.png"></p><h4 id="研究方法与过程"><a href="#研究方法与过程" class="headerlink" title="研究方法与过程"></a>研究方法与过程</h4><p><img src="/images/15/3-2.png"><br><img src="/images/15/3-3.png"><br><img src="/images/15/3-4.png"><br><img src="/images/15/3-5.png"><br><img src="/images/15/3-6.png"><br><img src="/images/15/3-7.png"><br><img src="/images/15/3-8.png"><br><img src="/images/15/3-9.png"></p><h4 id="研究成果的展示"><a href="#研究成果的展示" class="headerlink" title="研究成果的展示"></a>研究成果的展示</h4><p><img src="/images/15/3-10.png"><br><img src="/images/15/3-11.png"></p><h4 id="讨论与展望"><a href="#讨论与展望" class="headerlink" title="讨论与展望"></a>讨论与展望</h4><p><img src="/images/15/3-12.png"></p>]]></content>
      
      
      <categories>
          
          <category> 小组作业 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 多标签符包胸部X射线分类/高校录取数据爬虫实验 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/10/01/hello-world/"/>
      <url>/2023/10/01/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="quick-Start123123"><a href="#quick-Start123123" class="headerlink" title="quick Start123123"></a>quick Start123123</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> Hello World </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hello World </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
